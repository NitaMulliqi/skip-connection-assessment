{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Live_Pre-trained Resnet-Unet++.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "38CCqRT5wGjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install torch\n",
        "#!pip install torchvision\n",
        "#!pip install numpy\n",
        "#!pip install matplotlib\n",
        "#!pip install tqdm\n",
        "#!pip install tensorboardX\n",
        "#!pip install pretrainedmodels\n",
        "#%load_ext autoreload\n",
        "#%autoreload 2\n",
        "#%matplotlib inline\n",
        "import torchvision\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.backends import cudnn\n",
        "import random\n",
        "from random import shuffle\n",
        "import glob\n",
        "random.seed( 30 )\n",
        "import pprint\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils import data\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import pdb\n",
        "import collections\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import torch.nn as nn\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\"\"\"\n",
        "mp_lock = torch.multiprocessing.RLock()\n",
        "from torch.multiprocessing import Pool, Process, set_start_method\n",
        "try:\n",
        "    set_start_method('spawn')\n",
        "except RuntimeError:\n",
        "    pass\n",
        "\"\"\"\n",
        "\n",
        "writer_train_acc = SummaryWriter('/home/user3/myenv/result/Train_Acc')\n",
        "writer_valid_acc = SummaryWriter('/home/user3/myenv/result/Valid_Acc')\n",
        "writer_train_error = SummaryWriter('/home/user3/myenv/result/Train_Loss')\n",
        "writer_valid_error = SummaryWriter('/home/user3/myenv/result/Valid_Loss')\n",
        "writer_valid_dice = SummaryWriter('/home/user3/myenv/result/Valid_Dice')\n",
        "writer_train_dice = SummaryWriter('/home/user3/myenv/result/Train_Dice')\n",
        "writer_test_acc = SummaryWriter('/home/user3/myenv/result/Test_Acc')\n",
        "\n",
        "# tensorboard --logdir=/home/nita/anaconda3/Image_Segmentation/result\n",
        "# tensorboard --logdir=/home/nita/myenv/result\n",
        "# tensorboard --logdir=/home/colorlab/myenv/result\n",
        "# tensorboard --logdir=/home/user3/myenv/result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2xfaZpAwGjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3vOC-ukwGjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score:\n",
        "            self.counter += 1\n",
        "            print(\"EarlyStopping counter: {:.4f} out of {:.4f}\".format(self.counter, self.patience))\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(\"Validation loss decreased: {:.4f} --> {:.4f}\".format(self.val_loss_min, val_loss))\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds0HtbV8wGj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Pre-trained RESNet_Encoder\n",
        "\n",
        "%run resnet.ipynb\n",
        "import functools\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "encoders = {}\n",
        "encoders.update(resnet_encoders)\n",
        "\n",
        "def get_encoder(name, encoder_weights=None):\n",
        "    Encoder = encoders[name]['encoder']\n",
        "    encoder = Encoder(**encoders[name]['params'])\n",
        "    encoder.out_shapes = encoders[name]['out_shapes']\n",
        "\n",
        "    if encoder_weights is not None:\n",
        "        settings = encoders[name]['pretrained_settings'][encoder_weights]\n",
        "        encoder.load_state_dict(model_zoo.load_url(settings['url']))\n",
        "\n",
        "    return encoder\n",
        "\n",
        "def get_encoder_names():\n",
        "    return list(encoders.keys())\n",
        "\n",
        "def get_preprocessing_fn(encoder_name, pretrained='imagenet'):\n",
        "    settings = encoders[encoder_name]['pretrained_settings']\n",
        "\n",
        "    if pretrained not in settings.keys():\n",
        "        raise ValueError('Avaliable pretrained options {}'.format(settings.keys()))\n",
        "\n",
        "    input_space = settings[pretrained].get('input_space')\n",
        "    input_range = settings[pretrained].get('input_range')\n",
        "    mean = settings[pretrained].get('mean')\n",
        "    std = settings[pretrained].get('std')\n",
        "    \n",
        "    return functools.partial(preprocess_input, mean=mean, std=std, input_space=input_space, input_range=input_range)\n",
        "\n",
        "def preprocess_input(x, mean=None, std=None, input_space='RGB', input_range=None, **kwargs):\n",
        "\n",
        "    if input_space == 'BGR':\n",
        "        x = x[..., ::-1].copy()\n",
        "\n",
        "    if input_range is not None:\n",
        "        if x.max() > 1 and input_range[1] == 1:\n",
        "            x = x / 255.\n",
        "\n",
        "    if mean is not None:\n",
        "        mean = np.array(mean)\n",
        "        x = x - mean\n",
        "\n",
        "    if std is not None:\n",
        "        std = np.array(std)\n",
        "        x = x / std\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHBxuqVnwGj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class conv_block(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolution Block \n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(conv_block, self).__init__()\n",
        "        \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        " \n",
        "class up_conv(nn.Module):\n",
        "    \"\"\"\n",
        "    Up Convolution Block\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(up_conv, self).__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class U_Net_noskip(nn.Module):\n",
        "    \"\"\"\n",
        "    UNet - Basic Implementation\n",
        "    Paper : https://arxiv.org/abs/1505.04597\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=3, out_ch=1):\n",
        "        super(U_Net_noskip, self).__init__()\n",
        "\n",
        "        n1 = 64\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
        "        \n",
        "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.Conv1 = conv_block(in_ch, filters[0])\n",
        "        self.Conv2 = conv_block(filters[0], filters[1])\n",
        "        self.Conv3 = conv_block(filters[1], filters[2])\n",
        "        self.Conv4 = conv_block(filters[2], filters[3])\n",
        "        self.Conv5 = conv_block(filters[3], filters[4])\n",
        "\n",
        "        self.Up5 = up_conv(filters[4], filters[3])\n",
        "        self.Up_conv5 = conv_block(512, filters[3])\n",
        "\n",
        "        self.Up4 = up_conv(filters[3], filters[2])\n",
        "        self.Up_conv4 = conv_block(256, filters[2])\n",
        "\n",
        "        self.Up3 = up_conv(filters[2], filters[1])\n",
        "        self.Up_conv3 = conv_block(128, filters[1])\n",
        "\n",
        "        self.Up2 = up_conv(filters[1], filters[0])\n",
        "        self.Up_conv2 = conv_block(64, filters[0])\n",
        "\n",
        "        self.Conv = nn.Conv2d(filters[0], out_ch, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        #self.active = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        e1 = self.Conv1(x)\n",
        "\n",
        "        e2 = self.Maxpool1(e1)\n",
        "        e2 = self.Conv2(e2)\n",
        "\n",
        "        e3 = self.Maxpool2(e2)\n",
        "        e3 = self.Conv3(e3)\n",
        "\n",
        "        e4 = self.Maxpool3(e3)\n",
        "        e4 = self.Conv4(e4)\n",
        "\n",
        "        e5 = self.Maxpool4(e4)\n",
        "        e5 = self.Conv5(e5)\n",
        "\n",
        "        d5 = self.Up5(e5)\n",
        "        #print(d5.shape)\n",
        "        #d5 = torch.cat((e4, d5), dim=1)\n",
        "        #print(d5.shape)\n",
        "        d5 = self.Up_conv5(d5)\n",
        "\n",
        "        d4 = self.Up4(d5)\n",
        "        #d4 = torch.cat((e3, d4), dim=1)\n",
        "        d4 = self.Up_conv4(d4)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        #d3 = torch.cat((e2, d3), dim=1)\n",
        "        d3 = self.Up_conv3(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        #d2 = torch.cat((e1, d2), dim=1)\n",
        "        d2 = self.Up_conv2(d2)\n",
        "\n",
        "        out = self.Conv(d2)\n",
        "\n",
        "        #d1 = self.active(out)\n",
        "\n",
        "        return out\n",
        "    \n",
        "class U_Net(nn.Module):\n",
        "    \"\"\"\n",
        "    UNet - Basic Implementation\n",
        "    Paper : https://arxiv.org/abs/1505.04597\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=3, out_ch=1):\n",
        "        super(U_Net, self).__init__()\n",
        "\n",
        "        n1 = 64\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
        "        \n",
        "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.Conv1 = conv_block(in_ch, filters[0])\n",
        "        self.Conv2 = conv_block(filters[0], filters[1])\n",
        "        self.Conv3 = conv_block(filters[1], filters[2])\n",
        "        self.Conv4 = conv_block(filters[2], filters[3])\n",
        "        self.Conv5 = conv_block(filters[3], filters[4])\n",
        "\n",
        "        self.Up5 = up_conv(filters[4], filters[3])\n",
        "        self.Up_conv5 = conv_block(filters[4], filters[3])\n",
        "\n",
        "        self.Up4 = up_conv(filters[3], filters[2])\n",
        "        self.Up_conv4 = conv_block(filters[3], filters[2])\n",
        "\n",
        "        self.Up3 = up_conv(filters[2], filters[1])\n",
        "        self.Up_conv3 = conv_block(filters[2], filters[1])\n",
        "\n",
        "        self.Up2 = up_conv(filters[1], filters[0])\n",
        "        self.Up_conv2 = conv_block(filters[1], filters[0])\n",
        "\n",
        "        self.Conv = nn.Conv2d(filters[0], out_ch, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        #self.active = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        e1 = self.Conv1(x)\n",
        "\n",
        "        e2 = self.Maxpool1(e1)\n",
        "        e2 = self.Conv2(e2)\n",
        "\n",
        "        e3 = self.Maxpool2(e2)\n",
        "        e3 = self.Conv3(e3)\n",
        "\n",
        "        e4 = self.Maxpool3(e3)\n",
        "        e4 = self.Conv4(e4)\n",
        "\n",
        "        e5 = self.Maxpool4(e4)\n",
        "        e5 = self.Conv5(e5)\n",
        "\n",
        "        d5 = self.Up5(e5)\n",
        "        d5 = torch.cat((e4, d5), dim=1)\n",
        "\n",
        "        d5 = self.Up_conv5(d5)\n",
        "\n",
        "        d4 = self.Up4(d5)\n",
        "        d4 = torch.cat((e3, d4), dim=1)\n",
        "        d4 = self.Up_conv4(d4)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        d3 = torch.cat((e2, d3), dim=1)\n",
        "        d3 = self.Up_conv3(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        d2 = torch.cat((e1, d2), dim=1)\n",
        "        d2 = self.Up_conv2(d2)\n",
        "\n",
        "        out = self.Conv(d2)\n",
        "\n",
        "        #d1 = self.active(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Recurrent_block(nn.Module):\n",
        "    \"\"\"\n",
        "    Recurrent Block for R2Unet_CNN\n",
        "    \"\"\"\n",
        "    def __init__(self, out_ch, t=2):\n",
        "        super(Recurrent_block, self).__init__()\n",
        "\n",
        "        self.t = t\n",
        "        self.out_ch = out_ch\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self.t):\n",
        "            if i == 0:\n",
        "                x = self.conv(x)\n",
        "            out = self.conv(x + x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class RRCNN_block(nn.Module):\n",
        "    \"\"\"\n",
        "    Recurrent Residual Convolutional Neural Network Block\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, out_ch, t=2):\n",
        "        super(RRCNN_block, self).__init__()\n",
        "\n",
        "        self.RCNN = nn.Sequential(\n",
        "            Recurrent_block(out_ch, t=t),\n",
        "            Recurrent_block(out_ch, t=t)\n",
        "        )\n",
        "        self.Conv = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.Conv(x)\n",
        "        x2 = self.RCNN(x1)\n",
        "        out = x1 + x2\n",
        "        return out\n",
        "\n",
        "\n",
        "class R2U_Net(nn.Module):\n",
        "    \"\"\"\n",
        "    R2U-Unet implementation\n",
        "    Paper: https://arxiv.org/abs/1802.06955\n",
        "    \"\"\"\n",
        "    def __init__(self, img_ch=3, output_ch=1, t=2):\n",
        "        super(R2U_Net, self).__init__()\n",
        "\n",
        "        n1 = 64\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
        "\n",
        "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.Upsample = nn.Upsample(scale_factor=2)\n",
        "\n",
        "        self.RRCNN1 = RRCNN_block(img_ch, filters[0], t=t)\n",
        "\n",
        "        self.RRCNN2 = RRCNN_block(filters[0], filters[1], t=t)\n",
        "\n",
        "        self.RRCNN3 = RRCNN_block(filters[1], filters[2], t=t)\n",
        "\n",
        "        self.RRCNN4 = RRCNN_block(filters[2], filters[3], t=t)\n",
        "\n",
        "        self.RRCNN5 = RRCNN_block(filters[3], filters[4], t=t)\n",
        "\n",
        "        self.Up5 = up_conv(filters[4], filters[3])\n",
        "        self.Up_RRCNN5 = RRCNN_block(filters[4], filters[3], t=t)\n",
        "\n",
        "        self.Up4 = up_conv(filters[3], filters[2])\n",
        "        self.Up_RRCNN4 = RRCNN_block(filters[3], filters[2], t=t)\n",
        "\n",
        "        self.Up3 = up_conv(filters[2], filters[1])\n",
        "        self.Up_RRCNN3 = RRCNN_block(filters[2], filters[1], t=t)\n",
        "\n",
        "        self.Up2 = up_conv(filters[1], filters[0])\n",
        "        self.Up_RRCNN2 = RRCNN_block(filters[1], filters[0], t=t)\n",
        "\n",
        "        self.Conv = nn.Conv2d(filters[0], output_ch, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        #self.active = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        e1 = self.RRCNN1(x)\n",
        "\n",
        "        e2 = self.Maxpool(e1)\n",
        "        e2 = self.RRCNN2(e2)\n",
        "\n",
        "        e3 = self.Maxpool1(e2)\n",
        "        e3 = self.RRCNN3(e3)\n",
        "\n",
        "        e4 = self.Maxpool2(e3)\n",
        "        e4 = self.RRCNN4(e4)\n",
        "\n",
        "        e5 = self.Maxpool3(e4)\n",
        "        e5 = self.RRCNN5(e5)\n",
        "\n",
        "        d5 = self.Up5(e5)\n",
        "        d5 = torch.cat((e4, d5), dim=1)\n",
        "        d5 = self.Up_RRCNN5(d5)\n",
        "\n",
        "        d4 = self.Up4(d5)\n",
        "        d4 = torch.cat((e3, d4), dim=1)\n",
        "        d4 = self.Up_RRCNN4(d4)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        d3 = torch.cat((e2, d3), dim=1)\n",
        "        d3 = self.Up_RRCNN3(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        d2 = torch.cat((e1, d2), dim=1)\n",
        "        d2 = self.Up_RRCNN2(d2)\n",
        "\n",
        "        out = self.Conv(d2)\n",
        "\n",
        "      # out = self.active(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Attention_block(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        super(Attention_block, self).__init__()\n",
        "\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = self.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "        out = x * psi\n",
        "        return out\n",
        "\n",
        "class AttU_Net(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Unet implementation\n",
        "    Paper: https://arxiv.org/abs/1804.03999\n",
        "    \"\"\"\n",
        "    def __init__(self, img_ch=3, output_ch=1):\n",
        "        super(AttU_Net, self).__init__()\n",
        "        self.encoder = get_encoder('resnet50', encoder_weights='imagenet')\n",
        "        n1 = 64\n",
        "        filters = [n1, n1 * 4, n1 * 8, n1 * 16, n1 * 32]\n",
        "\n",
        "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.Conv1 = conv_block(img_ch, filters[0])\n",
        "        self.Conv2 = conv_block(filters[0], filters[1])\n",
        "        self.Conv3 = conv_block(filters[1], filters[2])\n",
        "        self.Conv4 = conv_block(filters[2], filters[3])\n",
        "        self.Conv5 = conv_block(filters[3], filters[4])\n",
        "\n",
        "        self.Up5 = up_conv(filters[4], filters[3])\n",
        "        self.Att5 = Attention_block(F_g=filters[3], F_l=filters[3], F_int=filters[2])\n",
        "        self.Up_conv5 = conv_block(filters[4], filters[3])\n",
        "\n",
        "        self.Up4 = up_conv(filters[3], filters[2])\n",
        "        self.Att4 = Attention_block(F_g=filters[2], F_l=filters[2], F_int=filters[1])\n",
        "        self.Up_conv4 = conv_block(filters[3], filters[2])\n",
        "\n",
        "        self.Up3 = up_conv(filters[2], filters[1])\n",
        "        self.Att3 = Attention_block(F_g=filters[1], F_l=filters[1], F_int=filters[0])\n",
        "        self.Up_conv3 = conv_block(filters[2], filters[1])\n",
        "\n",
        "        self.Up2 = up_conv(filters[1], filters[0])\n",
        "        self.Att2 = Attention_block(F_g=filters[0], F_l=filters[0], F_int=32)\n",
        "        self.Up_conv2 = conv_block(128, filters[0])\n",
        "        \n",
        "        self.Up_last = up_conv(filters[0], filters[0])\n",
        "\n",
        "        self.Conv = nn.Conv2d(filters[0], output_ch, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        #self.active = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        e1 = self.Conv1(x)\n",
        "        #print(e1.shape)\n",
        "        e2 = self.Maxpool1(e1)\n",
        "        e2 = self.Conv2(e2)\n",
        "        #print(e2.shape)\n",
        "        e3 = self.Maxpool2(e2)\n",
        "        e3 = self.Conv3(e3)\n",
        "        #print(e3.shape)\n",
        "        e4 = self.Maxpool3(e3)\n",
        "        e4 = self.Conv4(e4)\n",
        "        #print(e4.shape)\n",
        "        e5 = self.Maxpool4(e4)\n",
        "        e5 = self.Conv5(e5)\n",
        "        #print(e5.shape)\n",
        "        e5 = self.encoder(x)\n",
        "        \n",
        "        #print(e5[0].shape)\n",
        "        #print(e5[1].shape)\n",
        "        #print(e5[2].shape)\n",
        "        #print(e5[3].shape)\n",
        "        #print(e5[4].shape)\n",
        "        #print('nita')\n",
        "        #print(len(e5))\n",
        "        \n",
        "        d5 = self.Up5(e5[0])\n",
        "        x4 = self.Att5(g=d5, x=e5[1])\n",
        "        d5 = torch.cat((x4, d5), dim=1)\n",
        "        d5 = self.Up_conv5(d5)\n",
        "        #print(d5.shape)\n",
        "        \n",
        "        d4 = self.Up4(d5)\n",
        "        x3 = self.Att4(g=d4, x=e5[2])\n",
        "        d4 = torch.cat((x3, d4), dim=1)       \n",
        "        d4 = self.Up_conv4(d4)\n",
        "        #print(d4.shape) #512\n",
        "        \n",
        "        d3 = self.Up3(d4)\n",
        "        #print(d3.shape) #256\n",
        "        x2 = self.Att3(g=d3, x=e5[3])\n",
        "        #print(x2.shape) #256\n",
        "        d3 = torch.cat((x2, d3), dim=1)\n",
        "        #print(d3.shape) #512\n",
        "        d3 = self.Up_conv3(d3)\n",
        "        #print(d3.shape) #256\n",
        "        \n",
        "        d2 = self.Up2(d3)\n",
        "        #print(d2.shape) #64\n",
        "        x1 = self.Att2(g=d2, x=e5[4])\n",
        "        #print(e5[4].shape) #64\n",
        "        #print(x1.shape) #64\n",
        "        #print(d2.shape) #64\n",
        "        d2 = torch.cat((x1, d2), dim=1)\n",
        "        #print(d2.shape) # 128\n",
        "        d2 = self.Up_conv2(d2)\n",
        "        \n",
        "        #out = self.Conv(d2) old line of code\n",
        "    \n",
        "        #output is 64,112,112 - 64,224,224\n",
        "        out = self.Up_last(d2)\n",
        "        out = self.Conv(out)\n",
        "\n",
        "        #print(out.shape)\n",
        "\n",
        "        #out = self.active(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "class R2AttU_Net(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual Recuurent Block with attention Unet\n",
        "    Implementation : https://github.com/LeeJunHyun/Image_Segmentation\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=3, out_ch=1, t=2):\n",
        "        super(R2AttU_Net, self).__init__()\n",
        "\n",
        "        n1 = 64\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
        "\n",
        "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.RRCNN1 = RRCNN_block(in_ch, filters[0], t=t)\n",
        "        self.RRCNN2 = RRCNN_block(filters[0], filters[1], t=t)\n",
        "        self.RRCNN3 = RRCNN_block(filters[1], filters[2], t=t)\n",
        "        self.RRCNN4 = RRCNN_block(filters[2], filters[3], t=t)\n",
        "        self.RRCNN5 = RRCNN_block(filters[3], filters[4], t=t)\n",
        "        \n",
        "        self.Up5 = up_conv(filters[4], filters[3])\n",
        "        self.Att5 = Attention_block(F_g=filters[3], F_l=filters[3], F_int=filters[2])\n",
        "        self.Up_RRCNN5 = RRCNN_block(filters[4], filters[3], t=t)\n",
        "        \n",
        "        self.Up4 = up_conv(filters[3], filters[2])\n",
        "        self.Att4 = Attention_block(F_g=filters[2], F_l=filters[2], F_int=filters[1])\n",
        "        self.Up_RRCNN4 = RRCNN_block(filters[3], filters[2], t=t)\n",
        "        \n",
        "        self.Up3 = up_conv(filters[2], filters[1])\n",
        "        self.Att3 = Attention_block(F_g=filters[1], F_l=filters[1], F_int=filters[0])\n",
        "        self.Up_RRCNN3 = RRCNN_block(filters[2], filters[1], t=t)\n",
        "        \n",
        "        self.Up2 = up_conv(filters[1], filters[0])\n",
        "        self.Att2 = Attention_block(F_g=filters[0], F_l=filters[0], F_int=32)\n",
        "        self.Up_RRCNN2 = RRCNN_block(filters[1], filters[0], t=t)\n",
        "        \n",
        "        self.Conv = nn.Conv2d(filters[0], out_ch, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        #self.active = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        e1 = self.RRCNN1(x)\n",
        "\n",
        "        e2 = self.Maxpool1(e1)\n",
        "        e2 = self.RRCNN2(e2)\n",
        "\n",
        "        e3 = self.Maxpool2(e2)\n",
        "        e3 = self.RRCNN3(e3)\n",
        "\n",
        "        e4 = self.Maxpool3(e3)\n",
        "        e4 = self.RRCNN4(e4)\n",
        "\n",
        "        e5 = self.Maxpool4(e4)\n",
        "        e5 = self.RRCNN5(e5)\n",
        "\n",
        "        d5 = self.Up5(e5)\n",
        "        e4 = self.Att5(g=d5, x=e4)\n",
        "        d5 = torch.cat((e4, d5), dim=1)\n",
        "        d5 = self.Up_RRCNN5(d5)\n",
        "\n",
        "        d4 = self.Up4(d5)\n",
        "        e3 = self.Att4(g=d4, x=e3)\n",
        "        d4 = torch.cat((e3, d4), dim=1)\n",
        "        d4 = self.Up_RRCNN4(d4)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        e2 = self.Att3(g=d3, x=e2)\n",
        "        d3 = torch.cat((e2, d3), dim=1)\n",
        "        d3 = self.Up_RRCNN3(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        e1 = self.Att2(g=d2, x=e1)\n",
        "        d2 = torch.cat((e1, d2), dim=1)\n",
        "        d2 = self.Up_RRCNN2(d2)\n",
        "\n",
        "        out = self.Conv(d2)\n",
        "\n",
        "        #out = self.active(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "#For nested 3 channels are required\n",
        "\n",
        "class conv_block_nested(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(conv_block_nested, self).__init__()\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_ch)\n",
        "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        output = self.activation(x)\n",
        "\n",
        "        return output\n",
        "    \n",
        "#Nested Unet\n",
        "\n",
        "class NestedUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of this paper:\n",
        "    https://arxiv.org/pdf/1807.10165.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=3, out_ch=1):\n",
        "        super(NestedUNet, self).__init__()\n",
        "        self.encoder = get_encoder('resnet50', encoder_weights='imagenet')\n",
        "        n1 = 64\n",
        "        filters = [n1, n1 * 4, n1 * 8, n1 * 16, n1 * 32]\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        \n",
        "        #self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\n",
        "        #self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])\n",
        "        #self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])\n",
        "        #self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])\n",
        "        #self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])\n",
        "        \n",
        "        self.conv0_1 = conv_block_nested(filters[0] + filters[1], filters[0], filters[0]) #(64+256, 64, 64)\n",
        "        self.conv0_2 = conv_block_nested(filters[0]*2 + filters[1], filters[0], filters[0]) #(64*2+256, 64, 64)\n",
        "        self.conv0_3 = conv_block_nested(filters[0]*3 + filters[1], filters[0], filters[0]) #(64*3+256, 64, 64)\n",
        "             \n",
        "        self.conv1_1 = conv_block_nested(filters[1] + filters[2], filters[1], filters[1]) #\n",
        "        self.conv1_2 = conv_block_nested(filters[1]*2 + filters[2], filters[1], filters[1]) #\n",
        "        self.conv1_3 = conv_block_nested(filters[1]*3 + filters[2], filters[1], filters[1]) #   \n",
        "        \n",
        "        self.conv2_1 = conv_block_nested(filters[2] + filters[3], filters[2], filters[2])\n",
        "        self.conv2_2 = conv_block_nested(filters[2]*2 + filters[3], filters[2], filters[2])\n",
        "        \n",
        "        self.conv3_1 = conv_block_nested(filters[3] + filters[4], filters[3], filters[3])\n",
        "        \n",
        "        self.conv0_4 = conv_block_nested(filters[0]*4 + filters[1], filters[0], filters[0])\n",
        "        \n",
        "        self.final = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        e = self.encoder(x)\n",
        "        \n",
        "        #print(e[0].shape) #  1, 2048, 7, 7\n",
        "        #print(e[1].shape) #  1, 1024, 14, 14\n",
        "        #print(e[2].shape) #  1, 512, 28, 28\n",
        "        #print(e[3].shape) #  1, 256, 56, 56\n",
        "        #print(e[4].shape) #  1, 64, 112, 112 #after upsampling it is 64x224x224\n",
        "        \n",
        "        x0_0 = self.Up(e[4])\n",
        "        x1_0 = e[3]\n",
        "        x2_0 = e[2]\n",
        "        x3_0 = e[1]\n",
        "        x4_0 = e[0]\n",
        "        \n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0, self.Up(self.Up(x1_0))], 1))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0, self.Up(x2_0)], 1))\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0, self.Up(x3_0)], 1))\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0, self.Up(x4_0)], 1))\n",
        "        \n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.Up(x3_1)], 1))\n",
        "        \n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.Up(x2_1)], 1))\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.Up(x2_2)], 1))\n",
        "        \n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.Up(self.Up(x1_1))], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.Up(self.Up(x1_2))], 1))        \n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.Up(self.Up(x1_3))], 1))\n",
        "        \n",
        "        output = self.final(x0_4)\n",
        "       \n",
        "        return output\n",
        "\n",
        "#Modified Nested Unet with attention module\n",
        "class Modified_NestedUNet_Att(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch=3, out_ch=1):\n",
        "        super(Modified_NestedUNet_Att, self).__init__()\n",
        "        \n",
        "        self.encoder = get_encoder('resnet50', encoder_weights='imagenet')\n",
        "        \n",
        "        n1 = 64\n",
        "        filters = [n1, n1 * 4, n1 * 8, n1 * 16, n1 * 32]\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        \n",
        "        self.conv0_1 = conv_block_nested(filters[0] + filters[1], filters[0], filters[0]) #(64+256, 64, 64)\n",
        "        self.conv0_2 = conv_block_nested(filters[0]*2 + filters[1] + filters[2], filters[0], filters[0]) #(64*2+256*2, 64, 64)\n",
        "        self.conv0_3 = conv_block_nested(filters[0]*3 + filters[1] + filters[2] + filters[3], filters[0], filters[0]) #(64*3+256, 64, 64)\n",
        "        \n",
        "        self.conv1_1 = conv_block_nested(filters[1] + filters[2], filters[1], filters[1])\n",
        "        self.conv1_2 = conv_block_nested(filters[1]*2 + filters[2] + filters[3], filters[1], filters[1])\n",
        "        self.conv1_3 = conv_block_nested(filters[1]*3 + filters[2]+256, filters[1], filters[1])\n",
        "        \n",
        "        self.conv2_1 = conv_block_nested(filters[2] + filters[3], filters[2], filters[2])\n",
        "        self.conv2_2 = conv_block_nested(filters[2]*2 + filters[3]+512, filters[2], filters[2])\n",
        "        \n",
        "        self.conv3_1 = conv_block_nested(filters[3] + filters[4]+1024, filters[3], filters[3])\n",
        "        \n",
        "        self.conv0_4 = conv_block_nested(filters[0]*4 + filters[1]+64, filters[0], filters[0])\n",
        "        \n",
        "        self.Up5 = up_conv(filters[4], filters[3])\n",
        "        self.Att5 = Attention_block(F_g=filters[3], F_l=filters[3], F_int=filters[2])\n",
        "        \n",
        "        self.Up4 = up_conv(filters[3], filters[2])\n",
        "        self.Att4 = Attention_block(F_g=filters[2], F_l=filters[2], F_int=filters[1])\n",
        "        \n",
        "        self.Up3 = up_conv(filters[2], filters[1])\n",
        "        self.Att3 = Attention_block(F_g=filters[1], F_l=filters[1], F_int=filters[0])\n",
        "        \n",
        "        self.Up2 = up_conv(filters[1], filters[0])\n",
        "        self.Att2 = Attention_block(F_g=filters[0], F_l=filters[0], F_int=32)\n",
        "        \n",
        "        self.final = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        e = self.encoder(x)\n",
        "        \n",
        "        #print(e[0].shape) #  1, 2048, 7, 7\n",
        "        #print(e[1].shape) #  1, 1024, 14, 14\n",
        "        #print(e[2].shape) #  1, 512, 28, 28\n",
        "        #print(e[3].shape) #  1, 256, 56, 56\n",
        "        #print(e[4].shape) #  1, 64, 112, 112 #after upsampling it is 64x224x224\n",
        "        \n",
        "        x0_0 = self.Up(e[4])\n",
        "        x1_0 = e[3]\n",
        "        x2_0 = e[2]\n",
        "        x3_0 = e[1]\n",
        "        x4_0 = e[0]\n",
        "        \n",
        "        d5 = self.Up5(x4_0)\n",
        "        x5 = self.Att5(g=d5, x=e[1])\n",
        "\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0, self.Up(self.Up(x1_0))], 1))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0, self.Up(x2_0)], 1))\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0, self.Up(x3_0)], 1))\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0, x5, self.Up(x4_0)], 1))\n",
        "\n",
        "        d4 = self.Up4(x3_1)\n",
        "        x4 = self.Att4(g=d4, x=x2_1)\n",
        "        \n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, x4, self.Up(x3_1)], 1))\n",
        "        \n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.Up(x2_1), self.Up(self.Up(x3_0)) ], 1))\n",
        "        \n",
        "        d3 = self.Up3(x2_2)\n",
        "        x3 = self.Att3(g=d3, x=x1_2)\n",
        "\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, x3, self.Up(x2_2)], 1))\n",
        "        \n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.Up(self.Up(x1_1)), self.Up(self.Up(self.Up(x2_0)))], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.Up(self.Up(x1_2)), self.Up(self.Up(self.Up(self.Up(x3_0)))), self.Up(self.Up(self.Up(x2_1)))], 1))\n",
        "\n",
        "        d2 = self.Up2(x1_3)\n",
        "        d2 = self.Up(d2)\n",
        "        x2 = self.Att2(g=d2, x=x0_3)\n",
        "        \n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, x2, self.Up(self.Up(x1_3))], 1))\n",
        "        \n",
        "        output = self.final(x0_4)\n",
        "       \n",
        "        return output\n",
        "\n",
        "## -------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "class conv_block_nested(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(conv_block_nested, self).__init__()\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_ch)\n",
        "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.activation(x)\n",
        "        output = self.dropout(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Modified Nested Unet\n",
        "class  Modified_NestedUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of this paper:\n",
        "    https://arxiv.org/pdf/1807.10165.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=3, out_ch=1):\n",
        "        super(Modified_NestedUNet, self).__init__()\n",
        "        \n",
        "        self.encoder = get_encoder('resnet50', encoder_weights='imagenet')\n",
        "        \n",
        "        n1 = 64\n",
        "        filters = [n1, n1 * 4, n1 * 8, n1 * 16, n1 * 32]\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.Up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.Up1 = nn.Upsample(size=224, mode='bilinear', align_corners=True)\n",
        "        self.Up2 = nn.Upsample(size=56, mode='bilinear', align_corners=True)\n",
        "        self.Up3 = nn.Upsample(size=28, mode='bilinear', align_corners=True)\n",
        "        \n",
        "        self.conv0_1 = conv_block_nested(filters[0] + filters[1], filters[0], filters[0]) #(64+256, 64, 64)\n",
        "        self.conv0_2 = conv_block_nested(filters[0]*2 + filters[1] + filters[2], filters[0], filters[0]) #(64*2+256*2, 64, 64)\n",
        "        self.conv0_3 = conv_block_nested(filters[0]*3 + filters[1] + filters[2] + filters[3], filters[0], filters[0]) #(64*3+256, 64, 64)\n",
        "        \n",
        "        self.conv1_1 = conv_block_nested(filters[1] + filters[2], filters[1], filters[1])\n",
        "        self.conv1_2 = conv_block_nested(filters[1]*2 + filters[2] + filters[3], filters[1], filters[1])\n",
        "        self.conv1_3 = conv_block_nested(filters[1]*3 + filters[2], filters[1], filters[1])\n",
        "        \n",
        "        self.conv2_1 = conv_block_nested(filters[2] + filters[3], filters[2], filters[2])\n",
        "        self.conv2_2 = conv_block_nested(filters[2]*2 + filters[3], filters[2], filters[2])        \n",
        "        self.conv3_1 = conv_block_nested(filters[3] + filters[4], filters[3], filters[3])\n",
        "        self.conv0_4 = conv_block_nested(filters[0]*4 + filters[1], filters[0], filters[0])\n",
        "        \n",
        "        self.final = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        e = self.encoder(x)\n",
        "        \n",
        "        x0_0 = self.Up(e[4])\n",
        "        x1_0 = e[3]\n",
        "        x2_0 = e[2]\n",
        "        x3_0 = e[1]\n",
        "        x4_0 = e[0]\n",
        "        \n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0, self.Up1(x1_0)], 1))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0, self.Up(x2_0)], 1))\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0, self.Up(x3_0)], 1))\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0, self.Up(x4_0)], 1))\n",
        "        \n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.Up(x3_1)], 1))       \n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.Up2(x2_1), self.Up2(x3_0)], 1))\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.Up(x2_2)], 1))\n",
        "        \n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.Up1(x1_1), self.Up1(x2_0)], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.Up1(x1_2), self.Up1(x3_0), self.Up1(x2_1)], 1)) \n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.Up1(x1_3)], 1))\n",
        "        \n",
        "        output = self.final(x0_4)\n",
        "       \n",
        "        return output\n",
        "\n",
        "class DUpsampling(nn.Module):\n",
        "    def __init__(self, inplanes, scale, num_class=64, pad=0):\n",
        "        super(DUpsampling, self).__init__()\n",
        "        ## W matrix\n",
        "        self.conv_w = nn.Conv2d(inplanes, num_class * scale * scale, kernel_size=1, padding = pad,bias=False)\n",
        "        ## P matrix\n",
        "        self.conv_p = nn.Conv2d(num_class * scale * scale, inplanes, kernel_size=1, padding = pad,bias=False)\n",
        "\n",
        "        self.scale = scale\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv_w(x)\n",
        "        N, C, H, W = x.size()\n",
        "\n",
        "        # N, W, H, C\n",
        "        x_permuted = x.permute(0, 3, 2, 1) \n",
        "\n",
        "        # N, W, H*scale, C/scale\n",
        "        x_permuted = x_permuted.contiguous().view((N, W, H * self.scale, int(C / (self.scale))))\n",
        "\n",
        "        # N, H*scale, W, C/scale\n",
        "        x_permuted = x_permuted.permute(0, 2, 1, 3)\n",
        "        # N, H*scale, W*scale, C/(scale**2)\n",
        "        x_permuted = x_permuted.contiguous().view((N, W * self.scale, H * self.scale, int(C / (self.scale * self.scale))))\n",
        "\n",
        "        # N, C/(scale**2), H*scale, W*scale\n",
        "        x = x_permuted.permute(0, 3, 1, 2)\n",
        "        \n",
        "        return x\n",
        "\n",
        "### Modified NestedUnet_V2\n",
        "class Modified_NestedUNet_V2(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of this paper:\n",
        "    https://arxiv.org/pdf/1807.10165.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=3, out_ch=1):\n",
        "        super(Modified_NestedUNet_V2, self).__init__()\n",
        "        \n",
        "        self.encoder = get_encoder('resnet50', encoder_weights='imagenet')\n",
        "        \n",
        "        n1 = 64\n",
        "        filters = [n1, n1 * 4, n1 * 8, n1 * 16, n1 * 32]\n",
        "\n",
        "        # Encoder\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.Up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.Up1 = nn.Upsample(size=224, mode='bilinear', align_corners=True)\n",
        "        self.Up2 = nn.Upsample(size=56, mode='bilinear', align_corners=True)\n",
        "        self.Up3 = nn.Upsample(size=28, mode='bilinear', align_corners=True)\n",
        "        \n",
        "        self.conv0_1 = conv_block_nested(filters[0] + filters[1], filters[0], filters[0]) #(64+256, 64, 64)\n",
        "        self.conv0_2 = conv_block_nested(filters[0]*2 + filters[1] + filters[2], filters[0], filters[0]) #(64*2+256*2, 64, 64)\n",
        "        self.conv0_3 = conv_block_nested(filters[0]*3 + filters[1] + filters[2] + filters[3], filters[0], filters[0]) #(64*3+256, 64, 64)\n",
        "        \n",
        "        self.conv1_1 = conv_block_nested(filters[1] + filters[2], filters[1], filters[1])\n",
        "        self.conv1_2 = conv_block_nested(filters[1]*2 + filters[2] + filters[3], filters[1], filters[1])\n",
        "        self.conv2_1 = conv_block_nested(filters[2] + filters[3], filters[2], filters[2])\n",
        "        \n",
        "        # Decoder\n",
        "        self.conv1 = nn.Conv2d(filters[4], filters[3], kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(filters[3])\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(filters[3], filters[2], kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(filters[2])\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(filters[2], filters[1], kernel_size=3, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(filters[1])\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.conv4 = nn.Conv2d(filters[1], filters[0], kernel_size=3, padding=1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(filters[0])\n",
        "        self.dropout4 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.dupsample4 = DUpsampling(filters[0], 32, num_class=64)\n",
        "        \n",
        "        self.conv6 = nn.Conv2d(filters[0]*2, filters[0], kernel_size=3, padding=1, bias=False)\n",
        "        self.bn6 = nn.BatchNorm2d(filters[0])\n",
        "        self.dropout6 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.final = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # Encoder\n",
        "        e = self.encoder(x)\n",
        "        #print(e[0].shape) #  1, 2048, 7, 7\n",
        "        #print(e[1].shape) #  1, 1024, 14, 14\n",
        "        #print(e[2].shape) #  1, 512, 28, 28\n",
        "        #print(e[3].shape) #  1, 256, 56, 56\n",
        "        #print(e[4].shape) #  1, 64, 112, 112 #after upsampling it is 64x224x224\n",
        "        \n",
        "        x0_0 = self.Up(e[4])\n",
        "        x1_0 = e[3]\n",
        "        x2_0 = e[2]\n",
        "        x3_0 = e[1]\n",
        "        x4_0 = e[0]\n",
        "        \n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0, self.Up1(x1_0)], 1))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0, self.Up(x2_0)], 1))\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0, self.Up(x3_0)], 1))\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.Up2(x2_1), self.Up2(x3_0)], 1))\n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.Up1(x1_1), self.Up1(x2_0)], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.Up1(x1_2), self.Up1(x3_0), self.Up1(x2_1)], 1)) \n",
        "        # x0_3 = 64 x 224 x 224\n",
        "        \n",
        "        # Decoder\n",
        "        x3_1 = self.conv1(x4_0)\n",
        "        x3_1 = self.bn1(x3_1)\n",
        "        x3_1 = self.relu(x3_1)\n",
        "        \n",
        "        x3_1 = self.conv2(x3_1)\n",
        "        x3_1 = self.bn2(x3_1)\n",
        "        x3_1 = self.relu(x3_1)\n",
        "        x3_1 = self.dropout2(x3_1)\n",
        "        \n",
        "        x3_1 = self.conv3(x3_1)\n",
        "        x3_1 = self.bn3(x3_1)\n",
        "        x3_1 = self.relu(x3_1)\n",
        "        x3_1 = self.dropout3(x3_1)\n",
        "        \n",
        "        x3_1 = self.conv4(x3_1)\n",
        "        x3_1 = self.bn4(x3_1)\n",
        "        x3_1 = self.relu(x3_1)\n",
        "        x3_1 = self.dropout4(x3_1)\n",
        "        \n",
        "        x3_1_up = self.dupsample4(x3_1)\n",
        "\n",
        "        x0_4_cat = torch.cat((x3_1_up, x0_3), dim=1)\n",
        "        \n",
        "        x3_1_final = self.conv6(x0_4_cat)\n",
        "        x3_1_final = self.bn6(x3_1_final)\n",
        "        x3_1_final = self.relu(x3_1_final)\n",
        "        x3_1_final = self.dropout6(x3_1_final)\n",
        "        \n",
        "        output = self.final(x3_1_final)\n",
        "       \n",
        "        return output\n",
        "    \n",
        "class NestedUNet_Old(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of this paper:\n",
        "    https://arxiv.org/pdf/1807.10165.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=3, out_ch=1):\n",
        "        super(NestedUNet_Old, self).__init__()\n",
        "        \n",
        "        self.encoder = get_encoder('resnet50', encoder_weights='imagenet')\n",
        "        \n",
        "        n1 = 64\n",
        "        filters = [n1, n1 * 4, n1 * 8, n1 * 16, n1 * 32]\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        \n",
        "        self.conv0_1 = conv_block_nested(filters[0] + filters[1], filters[0], filters[0]) #(64+256, 64, 64)\n",
        "        self.conv0_2 = conv_block_nested(filters[0]*2 + filters[1] + filters[2], filters[0], filters[0]) #(64*2+256*2, 64, 64)\n",
        "        self.conv0_3 = conv_block_nested(filters[0]*3 + filters[1] + filters[2] + filters[3], filters[0], filters[0]) #(64*3+256, 64, 64)\n",
        "        \n",
        "        self.conv1_1 = conv_block_nested(filters[1] + filters[2], filters[1], filters[1])\n",
        "        self.conv1_2 = conv_block_nested(filters[1]*2 + filters[2] + filters[3], filters[1], filters[1])\n",
        "        self.conv1_3 = conv_block_nested(filters[1]*3 + filters[2], filters[1], filters[1])\n",
        "        \n",
        "        self.conv2_1 = conv_block_nested(filters[2] + filters[3], filters[2], filters[2])\n",
        "        self.conv2_2 = conv_block_nested(filters[2]*2 + filters[3], filters[2], filters[2])\n",
        "        \n",
        "        self.conv3_1 = conv_block_nested(filters[3] + filters[4], filters[3], filters[3])\n",
        "        \n",
        "        self.conv0_4 = conv_block_nested(filters[0]*4 + filters[1], filters[0], filters[0])\n",
        "        \n",
        "        self.final = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        e = self.encoder(x)\n",
        "        \n",
        "        #print(e[0].shape) #  1, 2048, 7, 7\n",
        "        #print(e[1].shape) #  1, 1024, 14, 14\n",
        "        #print(e[2].shape) #  1, 512, 28, 28\n",
        "        #print(e[3].shape) #  1, 256, 56, 56\n",
        "        #print(e[4].shape) #  1, 64, 112, 112 #after upsampling it is 64x224x224\n",
        "        \n",
        "        x0_0 = self.Up(e[4])\n",
        "        x1_0 = e[3]\n",
        "        x2_0 = e[2]\n",
        "        x3_0 = e[1]\n",
        "        x4_0 = e[0]\n",
        "        \n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0, self.Up(self.Up(x1_0))], 1))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0, self.Up(x2_0)], 1))\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0, self.Up(x3_0)], 1))\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0, self.Up(x4_0)], 1))\n",
        "        \n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.Up(x3_1)], 1))\n",
        "        \n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.Up(x2_1), self.Up(self.Up(x3_0)) ], 1))\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.Up(x2_2)], 1))\n",
        "        \n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.Up(self.Up(x1_1)), self.Up(self.Up(self.Up(x2_0)))], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.Up(self.Up(x1_2)), self.Up(self.Up(self.Up(self.Up(x2_0)))), self.Up(self.Up(self.Up(x2_1)))], 1))        \n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.Up(self.Up(x1_3))], 1))\n",
        "        \n",
        "        output = self.final(x0_4)\n",
        "       \n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuoXqLJywGj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(AttU_Net(img_ch=3, output_ch=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yh6s4vQwGj8",
        "colab_type": "code",
        "outputId": "b2fec573-8964-4b16-ccae-f9e1c1de3aec",
        "colab": {}
      },
      "source": [
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "class ImageFolder(data.Dataset):\n",
        "  \n",
        "  def __init__(self, root, mode='train'):\n",
        "    \n",
        "    self.images_paths = []\n",
        "    self.GT_paths = []\n",
        "    self.image_size = 224\n",
        "    self.mode = mode\n",
        "    \n",
        "    if (self.mode == 'train'):\n",
        "      for folder_path in glob.glob(root):\n",
        "        for image_path in glob.glob(os.path.join(root + '/ShortVD*', 'ShortVD*[!*.wmv]', 'ShortVD*')):\n",
        "          self.images_paths.append(image_path)\n",
        "      for folder_path in glob.glob(root):\n",
        "        for GT_path in glob.glob(os.path.join(root + '/ShortVD*', 'GT', 'ShortVD*')):\n",
        "          self.GT_paths.append(GT_path)\n",
        "    \n",
        "    if (self.mode == 'test') or (self.mode == 'valid'):\n",
        "      for folder_path in glob.glob(root):\n",
        "        for image_path in glob.glob(os.path.join(root + '/testvid*', 'testvid*[!*.wmv]', 'p*')):\n",
        "          self.images_paths.append(image_path)\n",
        "\n",
        "      for folder_path in glob.glob(root):\n",
        "        for GT_path in glob.glob(os.path.join(root + '/testvid*', 'GT', 'p*')):\n",
        "          self.GT_paths.append(GT_path)\n",
        "\n",
        "    print(\"Image count in {} path :{}\".format(self.mode, len(self.images_paths)))\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    if(self.mode == 'train'):\n",
        "      # Extract paths corresponding path-image\n",
        "      image_path = self.images_paths[index]\n",
        "      splited_path = image_path.split(\"/\")\n",
        "      image_n = splited_path[-1]\n",
        "      gt_image_n = image_n[:-len('.tiff')] + '_GT.tiff'\n",
        "      half_gt_path = '/'.join(splited_path[0:-2])\n",
        "      GT_path = half_gt_path + '/GT/' + gt_image_n\n",
        "    \n",
        "    if(self.mode == 'test') or (self.mode == 'valid'):\n",
        "      # Extract paths corresponding path-image\n",
        "      image_path = self.images_paths[index]\n",
        "      splited_path = image_path.split(\"/\")\n",
        "      image_n = splited_path[-1]\n",
        "      gt_image_n = image_n[:-len('.tiff')] + '.tif'\n",
        "      half_gt_path = '/'.join(splited_path[0:-2])\n",
        "      GT_path = half_gt_path + '/GT/' + gt_image_n\n",
        "    \n",
        "    image = Image.open(image_path)\n",
        "    GT = Image.open(GT_path)\n",
        "    \n",
        "    p_transform = random.random()\n",
        "    \n",
        "    if (self.mode == 'train'):\n",
        "        \n",
        "        ###------- Affine transformations performed & saved before training\n",
        "        \"\"\"\n",
        "        ### Shear & Translate\n",
        "        image = F.affine(image, 0, translate=[-1, 1], scale=1, shear=25, resample=False, fillcolor=0)\n",
        "        GT = F.affine(GT, 0, translate=[1, 1], scale=1, shear=25, resample=False, fillcolor=0)\n",
        "        \n",
        "        ### Rotate\n",
        "        image = F.rotate(image, 350, resample=False, expand=False, center=None)\n",
        "        GT = F.rotate(GT, 350, resample=False, expand=False, center=None)\n",
        "    \n",
        "        image = F.rotate(image, 10, resample=False, expand=True, center=None)\n",
        "        GT = F.rotate(GT, 10, resample=False, expand=True, center=None)\n",
        "        \"\"\"\n",
        "        \n",
        "        ###----- Perspective transformation\n",
        "        if random.random() < 0.5:\n",
        "          image = F.hflip(image)\n",
        "          GT = F.hflip(GT)\n",
        "        \n",
        "        if random.random() < 0.4:\n",
        "          image = F.vflip(image)\n",
        "          GT = F.vflip(GT)\n",
        "    \n",
        "    image = image.resize((224,224), Image.ANTIALIAS)\n",
        "    image = np.array(image, dtype='uint8')\n",
        "    image = torch.from_numpy(image)\n",
        "    image = image.permute(2,0,1)\n",
        "    image = image.float()/255\n",
        "    \n",
        "    GT = GT.resize((224,224), Image.ANTIALIAS)\n",
        "    GT = np.array(GT, dtype='uint8')\n",
        "    GT = torch.from_numpy(GT)\n",
        "    GT [GT > 0] = 255.0\n",
        "    GT = GT.float()/255.0\n",
        "    \n",
        "    #unique, counts = np.unique(GT, return_counts=True)\n",
        "    #print(dict(zip(unique, counts)))\n",
        "    \n",
        "    Norm_ = T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    image = Norm_(image)\n",
        "    \n",
        "    return image, GT\n",
        "\n",
        "  def __len__(self):\n",
        "    #Returns the total number of font files\n",
        "    return len(self.images_paths)\n",
        "\n",
        "train_dataset = ImageFolder(root='/media/user3/My Passport/Nita/TrainingSet_NewGT', mode='train')\n",
        "valid_dataset = ImageFolder(root='/media/user3/My Passport/Nita/ValidationSet_NewGT', mode='valid')\n",
        "test_dataset = ImageFolder(root='/media/user3/My Passport/Nita/AsuMayoTest', mode='test')\n",
        "\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=6, shuffle=True, num_workers=8)\n",
        "valid_loader = data.DataLoader(dataset=valid_dataset, batch_size=6, shuffle=True, num_workers=8)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=6, shuffle=True, num_workers=8)\n",
        "\n",
        "\"\"\"\n",
        "dataset_size = len(train_dataset)\n",
        "validation_split = 0.2\n",
        "random_seed= 30\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "np.random.seed(random_seed)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=14, sampler=train_sampler, num_workers=8)\n",
        "valid_loader = data.DataLoader(dataset=train_dataset, batch_size=14, sampler=valid_sampler, num_workers=8)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=14, shuffle=False, num_workers=8)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image count in train path :22757\n",
            "Image count in valid path :4820\n",
            "Image count in test path :17574\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndataset_size = len(train_dataset)\\nvalidation_split = 0.2\\nrandom_seed= 30\\nindices = list(range(dataset_size))\\nsplit = int(np.floor(validation_split * dataset_size))\\nnp.random.seed(random_seed)\\nnp.random.shuffle(indices)\\ntrain_indices, val_indices = indices[split:], indices[:split]\\n\\ntrain_sampler = SubsetRandomSampler(train_indices)\\nvalid_sampler = SubsetRandomSampler(val_indices)\\n\\ntrain_loader = data.DataLoader(dataset=train_dataset, batch_size=14, sampler=train_sampler, num_workers=8)\\nvalid_loader = data.DataLoader(dataset=train_dataset, batch_size=14, sampler=valid_sampler, num_workers=8)\\ntest_loader = data.DataLoader(dataset=test_dataset, batch_size=14, shuffle=False, num_workers=8)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBYLpV-0wGkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_images(images, cls_true, cls_pred=None, smooth=True, filename='test.png'):\n",
        "    #pdb.set_trace()\n",
        "    assert len(images) == len(cls_true)\n",
        "    fig, axes = plt.subplots(5, 5,figsize=(60, 60))\n",
        "   \n",
        "    if cls_pred is None:\n",
        "        hspace = 0.6\n",
        "    else:\n",
        "        hspace = 0.9\n",
        "\n",
        "    fig.subplots_adjust(hspace=hspace, wspace=0.3)\n",
        "\n",
        "    # Interpolation type.\n",
        "    if smooth:\n",
        "        interpolation = 'spline16'\n",
        "    else:\n",
        "        interpolation = 'nearest'\n",
        "\n",
        "    count1 =0\n",
        "    count2 =0\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i < len(images)*2:\n",
        "            # Plot image.\n",
        "            if i % 2 ==0:\n",
        "                ax.imshow(np.uint8(images[count1]),interpolation=interpolation)\n",
        "                count1+= 1\n",
        "            else:\n",
        "                ax.imshow(np.uint8(cls_true[count2]),interpolation=interpolation,cmap=plt.get_cmap('gray'))\n",
        "                count2+= 1\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "  #  plt.rcParams[\"figure.figsize\"] = (60,60)\n",
        "    plt.savefig(filename,dpi=100)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVM2zAbowGkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image, gt = next(iter(train_loader));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIAKPSclwGkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "### Plot only one image\n",
        "def im_convert(tensor):\n",
        "    image = tensor.cpu().clone().detach().numpy()\n",
        "    image = image.transpose(1,2,0)\n",
        "    image = image*np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
        "    image = image.clip(0, 1)\n",
        "    return image\n",
        "\n",
        "#plt.imshow(im_convert(image[0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umWIcULQwGkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image = image.permute(0,2,3,1).numpy()\n",
        "image = image*np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
        "\n",
        "### Shfaq disa foto nga dataset\n",
        "#plot_images(image*255, gt.numpy()*255, cls_pred=None, smooth=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGexn9pCwGkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Plot gradients\n",
        "\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "def plot_grad_flow(named_parameters):\n",
        "    '''Plots the gradients flowing through different layers in the net during training.\n",
        "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
        "    \n",
        "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
        "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
        "    ave_grads = []\n",
        "    max_grads= []\n",
        "    layers = []\n",
        "    for n, p in named_parameters:\n",
        "        if(p.requires_grad) and (\"bias\" not in n):\n",
        "            layers.append(n)\n",
        "            ave_grads.append(p.grad.abs().mean())\n",
        "            max_grads.append(p.grad.abs().max())\n",
        "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
        "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
        "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
        "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "    plt.xlim(left=0, right=len(ave_grads))\n",
        "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
        "    plt.xlabel(\"Layers\")\n",
        "    plt.ylabel(\"average gradient\")\n",
        "    plt.title(\"Gradient flow\")\n",
        "    plt.grid(True)\n",
        "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
        "                Line2D([0], [0], color=\"b\", lw=4),\n",
        "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7Cvj3towGkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#======================= Detection evaluation =========================#\n",
        "\n",
        "# 1 indicates polyp; 0 indicates non-polyp\n",
        "def perimage(SR, GT):\n",
        "\n",
        "    tpd = fnd = fpd = tnd = 0\n",
        "\n",
        "    polyp_in_SR = 0 if sum(SR.flatten()) == 0 else 1\n",
        "    polyp_in_GT = 0 if sum(GT.flatten()) == 0 else 1\n",
        "    #print(sum(GT.flatten()))\n",
        "    #print(sum(SR.flatten()))\n",
        "    for i in range(1, polyp_in_SR + 1):\n",
        "        intersection = np.sum((SR[GT == 1] == i))\n",
        "        if intersection > 0:\n",
        "            tpd += 1\n",
        "        else:\n",
        "            fpd += 1\n",
        "\n",
        "    for i in range(1, polyp_in_GT + 1):\n",
        "        intersection = np.sum((GT[SR == 1] == i))\n",
        "        if intersection == 0:\n",
        "            fnd += 1\n",
        "\n",
        "    if polyp_in_GT == polyp_in_SR == 0:\n",
        "        tnd += 1\n",
        "\n",
        "    return tpd, tnd, fpd, fnd\n",
        "\n",
        "def detection_rate(SR, GT):\n",
        "    \n",
        "    SR.flatten()\n",
        "    GT.flatten()\n",
        "    SR = np.where(SR > 0.5, 1, 0)\n",
        "    GT = np.where(GT > 0, 1, 0)\n",
        "\n",
        "    SR = np.array(SR, dtype=np.double)\n",
        "    GT = np.array(GT, dtype=np.double)\n",
        "\n",
        "    #pdb.set_trace() \n",
        "    tpd = fnd = fpd = tnd = 0\n",
        "\n",
        "    for sr, gt in zip(SR, GT):\n",
        "        #pdb.set_trace()\n",
        "        tpdi, tndi, fpdi, fndi = perimage(sr, gt)\n",
        "        tpd = tpd + tpdi\n",
        "        fnd = fnd + fndi\n",
        "        tnd = tnd + tndi\n",
        "        fpd = fpd + fpdi\n",
        "\n",
        "    return tpd, tnd, fpd, fnd\n",
        "\n",
        "def detection_accuracy(tpd, tnd, fpd, fnd):\n",
        "    return ((tnd + tpd) / (tnd + tpd + fnd + fpd + 1e-20))\n",
        "\n",
        "def detection_recall(tpd, fnd):\n",
        "    return ((tpd) / (tpd + fnd + 1e-20))\n",
        "\n",
        "def detection_precision(tpd, fpd):\n",
        "    return ((tpd) / (tpd + fpd + 1e-20))\n",
        "\n",
        "def detection_dice_coefficient(tpd, fnd, fpd):\n",
        "    return ((2*tpd)/(2*tpd + fnd + fpd + 1e-20))\n",
        "\n",
        "#======================= Segmentation evaluation =======================#\n",
        "\n",
        "def segmentation_rate(SR, GT):\n",
        "    \n",
        "    SR.flatten()\n",
        "    GT.flatten()\n",
        "\n",
        "    SR = np.where(SR > 0.5, 1, 0)\n",
        "    GT = np.where(GT > 0, 1, 0)\n",
        "\n",
        "    SR = np.array(SR, dtype=np.double)\n",
        "    GT = np.array(GT, dtype=np.double)\n",
        "\n",
        "    #pdb.set_trace()\n",
        "    tp = np.sum(SR[GT == 1] == 1)\n",
        "    fp = np.sum(SR[GT == 0] == 1)\n",
        "    fn = np.sum(SR[GT == 1] == 0)\n",
        "    tn = np.sum(SR[GT == 0] == 0)\n",
        "    return tp, tn, fp, fn\n",
        "\n",
        "def segmentation_accuracy(tp, tn, fp, fn):\n",
        "    return ((tp + tn)/(tp + tn + fp + fn + 1e-20))\n",
        "\n",
        "def segmentation_recall(tp, fn):\n",
        "    return ((tp)/(tp + fn + 1e-20))\n",
        "\n",
        "def segmentation_precision(tp, fp):\n",
        "    return ((tp) / (tp + fp + 1e-20))\n",
        "\n",
        "def segmentation_dice_coefficient(tp, fn, fp):\n",
        "    return ((2*tp)/(2*tp + fn + fp + 1e-20))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKmVCnLhwGkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### EORROR: Foscal Loss + Dice loss\n",
        "\"\"\"\n",
        "def dice_loss(input, target):\n",
        "    #input = torch.sigmoid(input)\n",
        "    smooth = 1.0\n",
        "    input = input.squeeze()\n",
        "    target = target.squeeze()\n",
        "    iflat = input.view(-1)\n",
        "    tflat = target.view(-1)\n",
        "    intersection = (iflat * tflat).sum()\n",
        "    \n",
        "    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
        "  \n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        \n",
        "    def forward(self, input, target):\n",
        "        if not (target.size() == input.size()):\n",
        "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
        "                             .format(target.size(), input.size()))\n",
        "        input = input.squeeze()\n",
        "        target = target.squeeze()\n",
        "        max_val = (-input).clamp(min=0)\n",
        "        loss = input - input * target + max_val + \\\n",
        "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
        "\n",
        "        invprobs = torch.nn.functional.logsigmoid(-input * (target * 2.0 - 1.0))\n",
        "        loss = (invprobs * self.gamma).exp() * loss\n",
        "        \n",
        "        return loss.mean()\n",
        "\n",
        "class MixedLoss(nn.Module):\n",
        "    def __init__(self, alpha, gamma):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.focal = FocalLoss(gamma)Polyp\n",
        "        \n",
        "    def forward(self, input, target):\n",
        "        input = input.squeeze()\n",
        "        target = target.squeeze()\n",
        "        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n",
        "        return loss.mean()\n",
        "   \n",
        "### EORROR: BCE + DICE LOSS\n",
        "\n",
        "### Error loss function\n",
        "def bce_dice_loss(SR, GT):\n",
        "  \n",
        "    SR = SR.squeeze()\n",
        "    GT = GT.squeeze()\n",
        "    iflat = SR.view(-1)\n",
        "    tflat = GT.view(-1)\n",
        "    \n",
        "    #pdb.set_trace()\n",
        "    \n",
        "    criterion = torch.nn.BCELoss()\n",
        "    loss = criterion(iflat, tflat) + dice_loss(SR, GT)\n",
        "    return loss\n",
        "\"\"\"\n",
        "\n",
        "def dice_loss(SR, GT):\n",
        "    #input = torch.sigmoid(input)\n",
        "    smooth = 1.\n",
        "  \n",
        "    SR = SR.squeeze()\n",
        "    GT = GT.squeeze()\n",
        "    SR_probs = torch.nn.functional.sigmoid(SR)\n",
        "    iflat = SR_probs.view(-1)\n",
        "    tflat = GT.view(-1)\n",
        "    \n",
        "    intersection = (iflat * tflat).sum()\n",
        "    \n",
        "    return 1 - ((2. * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
        "\n",
        "def weighted_bce(SR, GT):\n",
        "    \n",
        "    SR = SR.squeeze()\n",
        "    GT = GT.squeeze()\n",
        "    \n",
        "    iflat = SR.view(-1)\n",
        "    tflat = GT.view(-1)\n",
        "    SR_dim = iflat.shape\n",
        "    \n",
        "    pos_weight = 100*torch.ones((SR_dim)).view(-1).cuda()\n",
        "    pos_weight = pos_weight.squeeze()\n",
        "    \n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    \n",
        "    loss = criterion(iflat, tflat) + dice_loss(SR, GT)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "def R_custom_penalization(SR, GT, limit=1.2):\n",
        "        \n",
        "    SR = (np.squeeze(SR)).cpu().detach().numpy()\n",
        "    GT = (np.squeeze(GT)).cpu().detach().numpy()\n",
        "    \n",
        "    SR = np.array(SR, dtype=np.double)\n",
        "    GT = np.array(GT, dtype=np.double)\n",
        "    \n",
        "    SR = np.where(SR > 0.5, 1, 0)\n",
        "    GT = np.where(GT > 0, 1, 0)\n",
        "\n",
        "    value = 1\n",
        "    # cuda = torch.device('cuda')\n",
        "    # *torch.tensor(R_custom_penalization(iflat, tflat, limit=1.2)).cuda()\n",
        "    polyp_in_SR = 0 if sum(SR.flatten()) == 0 else 1\n",
        "    polyp_in_GT = 0 if sum(GT.flatten()) == 0 else 1\n",
        "    \n",
        "    for i in range(1, polyp_in_GT + 1):\n",
        "        intersection = np.sum((GT[SR == 1] == i))\n",
        "        if intersection == 0:\n",
        "            value = limit \n",
        "\n",
        "    return value "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeg_tU2gwGkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/home/nita/anaconda3/lin/python3.7\n",
        "\n",
        "class Solver(object):\n",
        "  \n",
        "  def __init__(self, train_loader, valid_loader, test_loader):\n",
        "    \n",
        "    # Data loader\n",
        "    self.train_loader = train_loader\n",
        "    self.valid_loader = valid_loader\n",
        "    self.test_loader = test_loader\n",
        "    \n",
        "    # Models\n",
        "    self.attUnet = None\n",
        "    self.img_ch = 3\n",
        "    self.output_ch = 1\n",
        "    self.checkpoint_epoch = 0\n",
        "    \n",
        "    # Hyper-parameters\n",
        "    self.lr = 0.0001\n",
        "    self.beta1 = 0.9\n",
        "    self.beta2 =0.999\n",
        "    \n",
        "    # Training settings\n",
        "    self.num_epochs = 3\n",
        "    self.batch_size = 6\n",
        "    \n",
        "    # Paths\n",
        "    self.model_path = '/home/user3/myenv/models'\n",
        "    self.checkpoint_path = '/home/user3/myenv/checkpoints/checkpoint.pt'\n",
        "    \n",
        "    self.mode = 'train'\n",
        "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model_type = 'Modified_NestedUNet_V2'\n",
        "    self.t = 2\n",
        "    self.build_model()\n",
        "\n",
        "    #weight_initialization = init_weights(self.attUnet, init_type='xavier', gain=0.02)\n",
        "    \n",
        "    #self.early_stopping = EarlyStopping(patience=10, verbose=True)\n",
        "    \n",
        "    #attUnet_path = os.path.join(self.model_path, 'Model_ R2AttU_Net-50-0.0001-4.pkl')\n",
        "    #self.attUnet.load_state_dict(torch.load(attUnet_path))\n",
        "    #print('%s is Successfully Loaded from %s' % (self.model_type, attUnet_path))\n",
        "    \n",
        "    #self.attUnet = self.load_checkpoint(self.checkpoint_path)\n",
        "    \n",
        "  def build_model(self):\n",
        "\n",
        "    if self.model_type == 'U_Net':\n",
        "      self.attUnet = nn.DataParallel(U_Net(in_ch=3, out_ch=1))\n",
        "    if self.model_type == 'Net':\n",
        "        self.attUnet = Net()\n",
        "    elif self.model_type == 'AttU_Net':\n",
        "      self.attUnet = AttU_Net(img_ch=3, output_ch=1)\n",
        "    elif self.model_type == 'R2AttU_Net':\n",
        "      self.attUnet = R2AttU_Net(in_ch=3, out_ch=1, t=self.t)\n",
        "    elif self.model_type == 'NestedUNet':\n",
        "      self.attUnet =  nn.DataParallel(NestedUNet(in_ch=3, out_ch=1))\n",
        "    elif self.model_type == 'Modified_NestedUNet':\n",
        "      self.attUnet =  nn.DataParallel(Modified_NestedUNet(in_ch=3, out_ch=1))\n",
        "    elif self.model_type == 'Modified_NestedUNet_Att':\n",
        "      self.attUnet =  nn.DataParallel(Modified_NestedUNet_Att(in_ch=3, out_ch=1))\n",
        "    elif self.model_type == 'Modified_NestedUNet_V2':\n",
        "      self.attUnet =  nn.DataParallel(Modified_NestedUNet_V2(in_ch=3, out_ch=1))\n",
        "    elif self.model_type == 'NestedUNet_Old':\n",
        "      self.attUnet =  nn.DataParallel(NestedUNet_Old(in_ch=3, out_ch=1))\n",
        "    elif self.model_type == 'U_Net_noskip':\n",
        "      self.attUnet =  nn.DataParallel(U_Net_noskip(in_ch=3, out_ch=1))\n",
        "    \n",
        "    \"\"\"\n",
        "    my_list = ['self.attUnet.module.encoder.conv1.weight'\n",
        "    'self.attUnet.module.encoder.bn1.weight'\n",
        "    'self.attUnet.module.encoder.bn1.bias'\n",
        "    'self.attUnet.module.encoder.layer1.0.conv1.weight'\n",
        "    'self.attUnet.module.encoder.layer1.0.bn1.weight'\n",
        "    'self.attUnet.module.encoder.layer1.0.bn1.bias'\n",
        "    'self.attUnet.module.encoder.layer1.0.conv2.weight'\n",
        "    'self.attUnet.module.encoder.layer1.0.bn2.weight'\n",
        "    'self.attUnet.module.encoder.layer1.0.bn2.bias'\n",
        "    'self.attUnet.module.encoder.layer1.0.conv3.weight'\n",
        "    'self.attUnet.module.encoder.layer1.0.bn3.weight'\n",
        "    'self.attUnet.module.encoder.layer1.0.bn3.bias'\n",
        "    'self.attUnet.module.encoder.layer1.0.downsample.0.weight'\n",
        "    'self.attUnet.module.encoder.layer1.0.downsample.1.weight'\n",
        "    'self.attUnet.module.encoder.layer1.0.downsample.1.bias'\n",
        "    'self.attUnet.module.encoder.layer1.1.conv1.weight'\n",
        "    'self.attUnet.module.encoder.layer1.1.bn1.weight'\n",
        "    'self.attUnet.module.encoder.layer1.1.bn1.bias'\n",
        "    'self.attUnet.module.encoder.layer1.1.conv2.weight'\n",
        "    'self.attUnet.module.encoder.layer1.1.bn2.weight'\n",
        "    'self.attUnet.module.encoder.layer1.1.bn2.bias'\n",
        "    'self.attUnet.module.encoder.layer1.1.conv3.weight'\n",
        "    'self.attUnet.module.encoder.layer1.1.bn3.weight'\n",
        "    'self.attUnet.module.encoder.layer1.1.bn3.bias'\n",
        "    'self.attUnet.module.encoder.layer1.2.conv1.weight'\n",
        "    'self.attUnet.module.encoder.layer1.2.bn1.weight'\n",
        "    'self.attUnet.module.encoder.layer1.2.bn1.bias'\n",
        "    'self.attUnet.module.encoder.layer1.2.conv2.weight'\n",
        "    'self.attUnet.module.encoder.layer1.2.bn2.weight'\n",
        "    'self.attUnet.module.encoder.layer1.2.bn2.bias'\n",
        "    'self.attUnet.module.encoder.layer1.2.conv3.weight'\n",
        "    'self.attUnet.module.encoder.layer1.2.bn3.weight'\n",
        "    'self.attUnet.module.encoder.layer1.2.bn3.bias'\n",
        "    'self.attUnet.module.encoder.layer2.0.conv1.weight'\n",
        "    'self.attUnet.module.encoder.layer2.0.bn1.weight'\n",
        "    'self.attUnet.module.encoder.layer2.0.bn1.bias'\n",
        "    'self.attUnet.module.encoder.layer2.0.conv2.weight'\n",
        "    'self.attUnet.module.encoder.layer2.0.bn2.weight'\n",
        "    'self.attUnet.module.encoder.layer2.0.bn2.bias'\n",
        "    'self.attUnet.module.encoder.layer2.0.conv3.weight'\n",
        "    'self.attUnet.module.encoder.layer2.0.bn3.weight'\n",
        "    'self.attUnet.module.encoder.layer2.0.bn3.bias'\n",
        "    'self.attUnet.module.encoder.layer2.0.downsample.0.weight'\n",
        "    'self.attUnet.module.encoder.layer2.0.downsample.1.weight'\n",
        "    'self.attUnet.module.encoder.layer2.0.downsample.1.bias'\n",
        "    'self.attUnet.module.encoder.layer2.1.conv1.weight'\n",
        "    'self.attUnet.module.encoder.layer2.1.bn1.weight'\n",
        "    'self.attUnet.module.encoder.layer2.1.bn1.bias'\n",
        "    'self.attUnet.module.encoder.layer2.1.conv2.weight'\n",
        "    'self.attUnet.module.encoder.layer2.1.bn2.weight'\n",
        "    'self.attUnet.module.encoder.layer2.1.bn2.bias'\n",
        "    'self.attUnet.module.encoder.layer2.1.conv3.weight'\n",
        "    'self.attUnet.module.encoder.layer2.1.bn3.weight'\n",
        "    'self.attUnet.module.encoder.layer2.1.bn3.bias'\n",
        "    'self.attUnet.module.encoder.layer2.2.conv1.weight'\n",
        "    'self.attUnet.module.encoder.layer2.2.bn1.weight'\n",
        "    'self.attUnet.module.encoder.layer2.2.bn1.bias'\n",
        "    'self.attUnet.module.encoder.layer2.2.conv2.weight'\n",
        "    'self.attUnet.module.encoder.layer2.2.bn2.weight'\n",
        "    'self.attUnet.module.encoder.layer2.2.bn2.bias'\n",
        "    'self.attUnet.module.encoder.layer2.2.conv3.weight'\n",
        "    'self.attUnet.module.encoder.layer2.2.bn3.weight'\n",
        "    'self.attUnet.module.encoder.layer2.2.bn3.bias'\n",
        "    'self.attUnet.module.encoder.layer2.3.conv1.weight'\n",
        "    'self.attUnet.module.encoder.layer2.3.bn1.weight'\n",
        "    'self.attUnet.module.encoder.layer2.3.bn1.bias'\n",
        "    'self.attUnet.module.encoder.layer2.3.conv2.weight'\n",
        "    'self.attUnet.module.encoder.layer2.3.bn2.weight'\n",
        "    'self.attUnet.module.encoder.layer2.3.bn2.bias'\n",
        "    'self.attUnet.module.encoder.layer2.3.conv3.weight'\n",
        "    'self.attUnet.module.encoder.layer2.3.bn3.weight'\n",
        "    'self.attUnet.module.encoder.layer2.3.bn3.bias'\n",
        "    'self.attUnet.module.encoder.layer3.0.conv1.weight'\n",
        "    'self.attUnet.module.encoder.layer3.0.bn1.weight'\n",
        "    'self.attUnet.module.encoder.layer3.0.bn1.bias'\n",
        "    'self.attUnet.module.encoder.layer3.0.conv2.weight'\n",
        "    'self.attUnet.module.encoder.layer3.0.bn2.weight'\n",
        "    'self.attUnet.module.encoder.layer3.0.bn2.bias'\n",
        "    'self.attUnet.module.encoder.layer3.0.conv3.weight'\n",
        "    'self.attUnet.module.encoder.layer3.0.bn3.weight'\n",
        "    'self.attUnet.module.encoder.layer3.0.bn3.bias'\n",
        "    'self.attUnet.module.encoder.layer3.0.downsample.0.weight'\n",
        "    'self.attUnet.module.encoder.layer3.0.downsample.1.weight'\n",
        "    'self.attUnet.module.encoder.layer3.0.downsample.1.bias'\n",
        "    'self.attUnet.module.encoder.layer3.1.conv1.weight'\n",
        "    'self.attUnet.module.encoder.layer3.1.bn1.weight'\n",
        "    'self.attUnet.module.encoder.layer3.1.bn1.bias'\n",
        "    'self.attUnet.module.encoder.layer3.1.conv2.weight'\n",
        "    'self.attUnet.module.encoder.layer3.1.bn2.weight'\n",
        "    'self.attUnet.module.encoder.layer3.1.bn2.bias'\n",
        "    'self.attUnet.module.encoder.layer3.1.conv3.weight'\n",
        "    'self.attUnet.module.encoder.layer3.1.bn3.weight'\n",
        "    'self.attUnet.module.encoder.layer3.1.bn3.bias'\n",
        "    'self.attUnet.module.encoder.layer3.2.conv1.weight'\n",
        "    'self.attUnet.module.encoder.layer3.2.bn1.weight'\n",
        "    'self.attUnet.module.encoder.layer3.2.bn1.bias'\n",
        "    'self.attUnet.module.encoder.layer3.2.conv2.weight'\n",
        "    'self.attUnet.module.encoder.layer3.2.bn2.weight'\n",
        "    'self.attUnet.module.encoder.layer3.2.bn2.bias'\n",
        "    'self.attUnet.module.encoder.layer3.2.conv3.weight'\n",
        "    'self.attUnet.module.encoder.layer3.2.bn3.weight'\n",
        "    'self.attUnet.module.encoder.layer3.2.bn3.bias'\n",
        "    'self.attUnet.module.encoder.layer3.3.conv1.weight'\n",
        "    'self.attUnet.module.encoder.layer3.3.bn1.weight'\n",
        "    'self.attUnet.module.encoder.layer3.3.bn1.bias'\n",
        "    'self.attUnet.module.encoder.layer3.3.conv2.weight'\n",
        "    'self.attUnet.module.encoder.layer3.3.bn2.weight'\n",
        "    'self.attUnet.module.encoder.layer3.3.bn2.bias'\n",
        "    'self.attUnet.module.encoder.layer3.3.conv3.weight'\n",
        "    'self.attUnet.module.encoder.layer3.3.bn3.weight'\n",
        "    'self.attUnet.module.encoder.layer3.3.bn3.bias'\n",
        "    'self.attUnet.module.encoder.layer3.4.conv1.weight'\n",
        "    'self.attUnet.module.encoder.layer3.4.bn1.weight'\n",
        "    'self.attUnet.module.encoder.layer3.4.bn1.bias'\n",
        "    'self.attUnet.module.encoder.layer3.4.conv2.weight'\n",
        "    'self.attUnet.module.encoder.layer3.4.bn2.weight'\n",
        "    'self.attUnet.module.encoder.layer3.4.bn2.bias'\n",
        "    'self.attUnet.module.encoder.layer3.4.conv3.weight'\n",
        "    'self.attUnet.module.encoder.layer3.4.bn3.weight'\n",
        "    'self.attUnet.module.encoder.layer3.4.bn3.bias'\n",
        "    'self.attUnet.module.encoder.layer3.5.conv1.weight'\n",
        "    'self.attUnet.module.encoder.layer3.5.bn1.weight'\n",
        "    'self.attUnet.module.encoder.layer3.5.bn1.bias'\n",
        "    'self.attUnet.module.encoder.layer3.5.conv2.weight'\n",
        "    'self.attUnet.module.encoder.layer3.5.bn2.weight'\n",
        "    'self.attUnet.module.encoder.layer3.5.bn2.bias'\n",
        "    'self.attUnet.module.encoder.layer3.5.conv3.weight'\n",
        "    'self.attUnet.module.encoder.layer3.5.bn3.weight'\n",
        "    'self.attUnet.module.encoder.layer3.5.bn3.bias'\n",
        "    'self.attUnet.module.encoder.layer4.0.conv1.weight'\n",
        "    'self.attUnet.module.encoder.layer4.0.bn1.weight'\n",
        "    'self.attUnet.module.encoder.layer4.0.bn1.bias'\n",
        "    'self.attUnet.module.encoder.layer4.0.conv2.weight'\n",
        "    'self.attUnet.module.encoder.layer4.0.bn2.weight'\n",
        "    'self.attUnet.module.encoder.layer4.0.bn2.bias'\n",
        "    'self.attUnet.module.encoder.layer4.0.conv3.weight'\n",
        "    'self.attUnet.module.encoder.layer4.0.bn3.weight'\n",
        "    'self.attUnet.module.encoder.layer4.0.bn3.bias'\n",
        "    'self.attUnet.module.encoder.layer4.0.downsample.0.weight'\n",
        "    'self.attUnet.module.encoder.layer4.0.downsample.1.weight'\n",
        "    'self.attUnet.module.encoder.layer4.0.downsample.1.bias'\n",
        "    'self.attUnet.module.encoder.layer4.1.conv1.weight'\n",
        "    'self.attUnet.module.encoder.layer4.1.bn1.weight'\n",
        "    'self.attUnet.module.encoder.layer4.1.bn1.bias'\n",
        "    'self.attUnet.module.encoder.layer4.1.conv2.weight'\n",
        "    'self.attUnet.module.encoder.layer4.1.bn2.weight'\n",
        "    'self.attUnet.module.encoder.layer4.1.bn2.bias'\n",
        "    'self.attUnet.module.encoder.layer4.1.conv3.weight'\n",
        "    'self.attUnet.module.encoder.layer4.1.bn3.weight'\n",
        "    'self.attUnet.module.encoder.layer4.1.bn3.bias'\n",
        "    'self.attUnet.module.encoder.layer4.2.conv1.weight'\n",
        "    'self.attUnet.module.encoder.layer4.2.bn1.weight'\n",
        "    'self.attUnet.module.encoder.layer4.2.bn1.bias'\n",
        "    'self.attUnet.module.encoder.layer4.2.conv2.weight'\n",
        "    'self.attUnet.module.encoder.layer4.2.bn2.weight'\n",
        "    'self.attUnet.module.encoder.layer4.2.bn2.bias'\n",
        "    'self.attUnet.module.encoder.layer4.2.conv3.weight'\n",
        "    'self.attUnet.module.encoder.layer4.2.bn3.weight'\n",
        "    'self.attUnet.module.encoder.layer4.2.bn3.bias']\n",
        "    \n",
        "    encoder_params = list(filter(lambda kv: kv[0] in my_list, self.attUnet.named_parameters()))\n",
        "      \n",
        "    self.optimizer = optim.Adam([\n",
        "    {\"params\": encoder_params, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_3.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_3.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_3.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_3.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_3.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_3.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_3.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_3.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_2.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_2.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_2.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_2.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_2.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_2.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_2.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_2.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv3_1.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv3_1.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv3_1.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv3_1.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv3_1.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv3_1.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv3_1.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv3_1.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_4.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_4.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_4.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_4.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_4.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_4.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_4.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_4.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.final.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.final.bias, \"lr\": 0.0001}], self.lr, [self.beta1, self.beta2])\n",
        "    \"\"\"\n",
        "    # When you want to update only a part of weights\n",
        "    #self.optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.attUnet.parameters()), self.lr, [self.beta1, self.beta2])\n",
        "    \n",
        "    #When you want to update default -NORMAL OPTIMIZATION\n",
        "    self.optimizer = optim.Adam(list(self.attUnet.parameters()), self.lr, [self.beta1, self.beta2])\n",
        "    self.attUnet.to(self.device)\n",
        "    self.print_network(self.attUnet, self.model_type)\n",
        "   \n",
        "  #Print out the network information\n",
        "  def print_network(self, model, name):\n",
        "    num_params = 0\n",
        "    for p in model.parameters():\n",
        "      num_params += p.numel()\n",
        "        #print(model)\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            print(name)\n",
        "    print(name)\n",
        "    print(\"The number of parameters: {}\".format(num_params))\n",
        "\n",
        "  def load_checkpoint(self, file_path):\n",
        "    \n",
        "    checkpoint = torch.load(file_path)\n",
        "    self.checkpoint_epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    self.attUnet.load_state_dict(checkpoint['model_state_dict'])\n",
        "    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    \n",
        "    print(\"Checkpoint was loaded\")\n",
        "    print(loss)\n",
        "    print(self.checkpoint_epoch)\n",
        "    \n",
        "    \"\"\"\n",
        "    for parameter in model.parameters():\n",
        "      parameter.requires_grad = False\n",
        "\n",
        "    model.eval()\n",
        "    \"\"\"\n",
        "    return self.attUnet\n",
        "\n",
        "  def train(self):\n",
        "    ### Train encoder, generator and discriminator\n",
        "    # range(self.checkpoint_epoch, self.num_epochs)\n",
        "    #range(self.num_epochs) - original\n",
        "\n",
        "    for epoch in range(self.checkpoint_epoch, self.num_epochs):\n",
        "      self.per_epoch_train(epoch)\n",
        "      #try:\n",
        "      self.per_epoch_valid(epoch)\n",
        "      #except EarlyStopError:\n",
        "       # break\n",
        "\n",
        "  def per_epoch_train(self,epoch):\n",
        "\n",
        "    avg_meter = defaultdict(float)\n",
        "    pbar = tqdm(enumerate(self.train_loader), total=len(self.train_loader), desc=\"Epoch {}\".format(epoch), ncols=0)\n",
        "    self.attUnet.train(True)\n",
        "    meter = {}\n",
        "    \n",
        "    acc_detection = 0.\n",
        "    recall_detection = 0.\n",
        "    precision_detection = 0.\n",
        "    DC_detection = 0.\n",
        "    \n",
        "    acc_segmentation = 0.\n",
        "    recall_segmentation = 0.\n",
        "    precision_segmentation = 0.\n",
        "    DC_segmentation = 0.\n",
        "    \n",
        "    length = 0\n",
        "    avg_loss_train = 0.\n",
        "    iterations = 0\n",
        "\n",
        "    for i, data in pbar:\n",
        "    \n",
        "      images = data[0]\n",
        "      GT = data[1]\n",
        "      \n",
        "      images, GT = Variable(images.cuda()), Variable(GT.cuda())\n",
        "      SR = self.attUnet(images)\n",
        "\n",
        "      length += images.size(0)      \n",
        "      iterations += 1\n",
        "      \n",
        "      #criterion = MixedLoss(alpha=0.25, gamma=2)\n",
        "      #loss = criterion(SR_probs, GT)\n",
        "      \n",
        "      #loss = bce_dice_loss(SR_probs, GT)\n",
        "\n",
        "      loss = weighted_bce(SR, GT)\n",
        "      avg_loss_train += loss.item()\n",
        "      \n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "      plot_grad_flow(self.attUnet.named_parameters())\n",
        "        \n",
        "      SR_probs = torch.nn.functional.sigmoid(SR)\n",
        "      SR_numpy = (np.squeeze(SR_probs).data).cpu().numpy()\n",
        "      GT_numpy = (np.squeeze(GT).data).cpu().numpy()\n",
        "      \n",
        "      #print(GT_numpy.shape)\n",
        "      #print(SR_numpy.shape)\n",
        "      \n",
        "      #print(GT_numpy.size)\n",
        "      #print(SR_numpy.size)\n",
        "      \n",
        "      #pdb.set_trace()\n",
        "      tpd, tnd, fpd, fnd = detection_rate(SR_numpy, GT_numpy)\n",
        "      tp, tn, fp, fn = segmentation_rate(SR_numpy, GT_numpy)\n",
        "      \n",
        "      acc_detection += detection_accuracy(tpd, tnd, fpd, fnd)\n",
        "      recall_detection += detection_recall(tpd, fnd)\n",
        "      precision_detection += detection_precision(tpd, fpd)\n",
        "      DC_detection += detection_dice_coefficient(tpd, fnd, fpd)\n",
        "      \n",
        "      acc_segmentation += segmentation_accuracy(tp, tn, fp, fn)\n",
        "      recall_segmentation += segmentation_recall(tp, fn)\n",
        "      precision_segmentation += segmentation_precision(tp, fp)\n",
        "      DC_segmentation += segmentation_dice_coefficient(tp, fn, fp)\n",
        "      \n",
        "      meter['loss'] = avg_loss_train\n",
        "      meter['acc_detection'] = acc_detection\n",
        "      meter['recall_detection'] = recall_detection\n",
        "      meter['precision_detection'] = precision_detection\n",
        "      meter['DC_detection'] = DC_detection\n",
        "      meter['acc_segmentation'] = acc_segmentation\n",
        "      meter['recall_segmentation'] = recall_segmentation\n",
        "      meter['precision_segmentation'] = precision_segmentation\n",
        "      meter['DC_segmentation'] = DC_segmentation\n",
        "      \n",
        "      for k, val in meter.items():\n",
        "        avg_meter[k] = val\n",
        "        pbar.set_postfix(**{k: \"{:.5f}\".format(v / (length)) for k, v in avg_meter.items()})\n",
        "    \n",
        "    avg_loss_train = avg_loss_train/iterations\n",
        "    \n",
        "    acc_detection = acc_detection/iterations\n",
        "    recall_detection = recall_detection/iterations\n",
        "    precision_detection = precision_detection/iterations\n",
        "    DC_detection = DC_detection/iterations\n",
        "    \n",
        "    acc_segmentation = acc_segmentation/iterations\n",
        "    recall_segmentation = recall_segmentation/iterations\n",
        "    precision_segmentation = precision_segmentation/iterations\n",
        "    DC_segmentation = DC_segmentation/iterations\n",
        "    \n",
        "    # Print the evaluation results\n",
        "    print('Epoch [%d/%d], Loss: %.4f, \\n[Training], DiceS: %.4f, DiceD: %.4f, AccuracyS: %.4f, AccuracyD: %.4f, RecallS: %.4f, RecallD: %.4f, PrecisionS: %.4f, PrecisionD: %.4f' % (\n",
        "      epoch+1, self.num_epochs, \\\n",
        "      avg_loss_train, \\\n",
        "      DC_segmentation, \\\n",
        "      DC_detection, \\\n",
        "      acc_segmentation, \\\n",
        "      acc_detection, \\\n",
        "      recall_segmentation, \\\n",
        "      recall_detection, \\\n",
        "      precision_segmentation, \\\n",
        "      precision_detection, \\\n",
        "      ))\n",
        "    \n",
        "    writer_train_error.add_scalar('Train LOSS', avg_loss_train, iterations)\n",
        "    writer_train_acc.add_scalar('Train Accuracy', acc_detection, iterations)\n",
        "    writer_train_dice.add_scalar('Train Dice', DC_detection, iterations)\n",
        "    \n",
        "    checkpoint = {'model_state_dict': self.attUnet.state_dict(),\n",
        "                  'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                  'loss': avg_loss_train,\n",
        "                  'epoch': epoch}\n",
        "    \n",
        "    torch.save(checkpoint, '/home/user3/myenv/checkpoints/checkpoint.pt')    \n",
        "\n",
        "  def per_epoch_valid(self,epoch):\n",
        "    self.attUnet.train(False)\n",
        "    self.attUnet.eval()\n",
        "    \n",
        "    acc_detection = 0.\n",
        "    recall_detection = 0.\n",
        "    precision_detection = 0.\n",
        "    DC_detection = 0.\n",
        "    \n",
        "    acc_segmentation = 0.\n",
        "    recall_segmentation = 0.\n",
        "    precision_segmentation = 0.\n",
        "    DC_segmentation = 0.\n",
        "    \n",
        "    avg_loss_valid = 0.\n",
        "    length = 0\n",
        "    iterations = 0\n",
        "    \n",
        "    for i, (images, GT) in enumerate(self.valid_loader):\n",
        "      \n",
        "      images = images.to(self.device)\n",
        "      GT = GT.to(self.device)\n",
        "      \n",
        "      with torch.no_grad():\n",
        "        SR = self.attUnet(images)\n",
        "      \n",
        "      length += images.size(0)\n",
        "      iterations += 1\n",
        "      \n",
        "      #criterion = MixedLoss(alpha=0.25, gamma=2)\n",
        "      #loss = criterion(SR_probs, GT)\n",
        "      \n",
        "      #loss = bce_dice_loss(SR_probs, GT)\n",
        "           \n",
        "      loss = weighted_bce(SR, GT)\n",
        "      avg_loss_valid += loss.item()\n",
        "        \n",
        "      SR_probs = torch.nn.functional.sigmoid(SR)\n",
        "      SR_numpy = (np.squeeze(SR).data).cpu().numpy()\n",
        "      GT_numpy = (np.squeeze(GT).data).cpu().numpy()\n",
        "      \n",
        "      tpd, tnd, fpd, fnd = detection_rate(SR_numpy, GT_numpy)\n",
        "      tp, tn, fp, fn = segmentation_rate(SR_numpy, GT_numpy)\n",
        "      \n",
        "      acc_detection += detection_accuracy(tpd, tnd, fpd, fnd)\n",
        "      recall_detection += detection_recall(tpd, fnd)\n",
        "      precision_detection += detection_precision(tpd, fpd)\n",
        "      DC_detection += detection_dice_coefficient(tpd, fnd, fpd)\n",
        "      acc_segmentation += segmentation_accuracy(tp, tn, fp, fn)\n",
        "      recall_segmentation += segmentation_recall(tp, fn)\n",
        "      precision_segmentation += segmentation_precision(tp, fp)\n",
        "      DC_segmentation += segmentation_dice_coefficient(tp, fn, fp)\n",
        "      \n",
        "    avg_loss_valid = avg_loss_valid/iterations\n",
        "    \n",
        "    acc_detection = acc_detection/iterations\n",
        "    recall_detection = recall_detection/iterations\n",
        "    precision_detection = precision_detection/iterations\n",
        "    DC_detection = DC_detection/iterations\n",
        "    \n",
        "    acc_segmentation = acc_segmentation/iterations\n",
        "    recall_segmentation = recall_segmentation/iterations\n",
        "    precision_segmentation = precision_segmentation/iterations\n",
        "    DC_segmentation = DC_segmentation/iterations\n",
        "    \n",
        "    print('Epoch [%d/%d], Loss: %.4f, \\n[Validation], DiceS: %.4f, DiceD: %.4f, AccuracyS: %.4f, AccuracyD: %.4f, RecallS: %.4f, RecallD: %.4f, PrecisionS: %.4f, PrecisionD: %.4f' % (\n",
        "      epoch+1, self.num_epochs, \\\n",
        "      avg_loss_valid, \\\n",
        "      DC_segmentation, \\\n",
        "      DC_detection, \\\n",
        "      acc_segmentation, \\\n",
        "      acc_detection, \\\n",
        "      recall_segmentation, \\\n",
        "      recall_detection, \\\n",
        "      precision_segmentation, \\\n",
        "      precision_detection, \\\n",
        "      ))\n",
        "    \n",
        "    writer_valid_error.add_scalar('Valid LOSS', avg_loss_valid, iterations)\n",
        "    writer_valid_acc.add_scalar('Valid Accuracy', acc_detection, iterations)\n",
        "    writer_valid_dice.add_scalar('Valid Dice', DC_detection, iterations)\n",
        "    \n",
        "    #self.early_stopping(avg_loss_valid, self.attUnet)\n",
        "\n",
        "    #if self.early_stopping.early_stop:\n",
        "      #print(\"Early stopping\")\n",
        "      #raise EarlyStopError\n",
        "\n",
        "    ### Save model to a new file\n",
        "    self.model_path = '/home/user3/myenv/models'\n",
        "    attUnet_path = os.path.join(self.model_path, 'Model: %s-%d-%.4f-%d.pkl' % (self.model_type, self.num_epochs, self.lr, self.batch_size))\n",
        "    torch.save(self.attUnet.state_dict(), attUnet_path)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdNGthaswGkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Solver(train_loader, valid_loader, test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsGTpEwNwGkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV1i4HTFwGke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Evaluation for one batch\n",
        "\n",
        "image, gt = next(iter(test_loader));\n",
        "\n",
        "\n",
        "imageTS = image\n",
        "\n",
        "image = image.permute(0,2,3,1).numpy()\n",
        "image = image*np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
        "\n",
        "### Plot images and groud truth\n",
        "plot_images(image*255, gt.numpy()*255, cls_pred=None, smooth=True)\n",
        "\n",
        "### Pass image to the model\n",
        "\n",
        "model.attUnet.train(False)\n",
        "model.attUnet.eval()\n",
        "\n",
        "imageTS = imageTS.to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    SR = model.attUnet(imageTS)\n",
        "\n",
        "SR_probs = torch.nn.functional.sigmoid(SR)\n",
        "\n",
        "temp = np.squeeze(SR_probs.permute(0,2,3,1)).detach().cpu().numpy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Beoat1wGki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Evaluation per image')\n",
        "\n",
        "tpd, tnd, fpd, fnd = detection_rate(temp, gt)\n",
        "tp, tn, fp, fn = segmentation_rate(temp, gt)\n",
        "\"\"\"\n",
        "print('\\n Segmentation results')\n",
        "acc_segmentation = segmentation_accuracy(tp, tn, fp, fn)\n",
        "print(acc_segmentation)\n",
        "recall_segmentation = segmentation_recall(tp, fn)\n",
        "print(recall_segmentation)\n",
        "precision_segmentation = segmentation_precision(tp, fp)\n",
        "print(precision_segmentation)\n",
        "DC_segmentation = segmentation_dice_coefficient(tp, fn, fp)\n",
        "print(DC_segmentation)\n",
        "\"\"\"\n",
        "print('\\nDetection results')\n",
        "acc_detection = detection_accuracy(tpd, tnd, fpd, fnd)\n",
        "print(\"Acc: {}\".format(acc_detection))\n",
        "recall_detection = detection_recall(tpd, fnd)\n",
        "print(\"Recall: {}\".format(recall_detection))\n",
        "precision_detection = detection_precision(tpd, fpd)\n",
        "print(\"Precision: {}\".format(precision_detection))\n",
        "DC_detection = detection_dice_coefficient(tpd, fnd, fpd)\n",
        "print(\"Dice: {}\".format(DC_detection))\n",
        "\n",
        "print('\\nDetection rates')\n",
        "print(\"True Positive: {}\".format(tpd))\n",
        "print(\"True Negative: {}\".format(tnd))\n",
        "print(\"False Positive: {}\".format(fpd))\n",
        "print(\"False Negative: {}\".format(fnd))\n",
        "\n",
        "\"\"\"\n",
        "print('\\n Segmentation rates')\n",
        "print(tp)\n",
        "print(tn)\n",
        "print(fp)\n",
        "print(fn)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpOVh7l4wGkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_images(image*255, temp*255, cls_pred=None, smooth=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArQ_EKoTwGkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------------Testing-----------------#\n",
        "\n",
        "model.attUnet.train(False)\n",
        "model.attUnet.eval()\n",
        "\n",
        "#attUnet_path = os.path.join(self.model_path, 'Net-100-0.0000-0-0.5000.pkl')\n",
        "#self.attUnet.load_state_dict(torch.load(attUnet_path))\n",
        "\n",
        "length = 0\n",
        "meter = {}\n",
        "\n",
        "acc_detection = 0.\n",
        "recall_detection = 0.\n",
        "precision_detection = 0.\n",
        "DC_detection = 0.\n",
        "acc_segmentation = 0.\n",
        "recall_segmentation = 0.\n",
        "precision_segmentation = 0.\n",
        "DC_segmentation = 0.\n",
        "F1_detection = 0.\n",
        "F2_detection = 0.\n",
        "tpd_=[]\n",
        "tnd_=[]\n",
        "fpd_=[]\n",
        "fnd_=[]\n",
        "tp_=[]\n",
        "tn_=[]\n",
        "fp_=[]\n",
        "fn_=[]\n",
        "TP_number = 0\n",
        "FP_number = 0\n",
        "FN_number = 0\n",
        "\n",
        "for  i, (images, GT) in enumerate(test_loader):\n",
        "\n",
        "    images = images.to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        SR = model.attUnet(images)\n",
        "        \n",
        "    SR_probs = torch.nn.functional.sigmoid(SR)\n",
        "    \n",
        "    length += images.size(0)\n",
        "\n",
        "    SR_numpy = (np.squeeze(SR_probs).data).cpu().numpy()\n",
        "    GT_numpy = (np.squeeze(GT).data).cpu().numpy()\n",
        "\n",
        "    tpd, tnd, fpd, fnd = detection_rate(SR_numpy, GT_numpy)\n",
        "    tp, tn, fp, fn = segmentation_rate(SR_numpy, GT_numpy)\n",
        "\n",
        "    tpd_.append(tpd) \n",
        "    tnd_.append(tnd)\n",
        "    fpd_.append(fpd)\n",
        "    fnd_.append(fnd)\n",
        "    tp_.append(tp)\n",
        "    tn_.append(tn)\n",
        "    fp_.append(fp)\n",
        "    fn_.append(fn)\n",
        "\n",
        "acc_detection = detection_accuracy(sum(tpd_), sum(tnd_), sum(fpd_), sum(fnd_))\n",
        "recall_detection = detection_recall(sum(tpd_), sum(fnd_))\n",
        "precision_detection = detection_precision(sum(tpd_), sum(fpd_))\n",
        "DC_detection = detection_dice_coefficient(sum(tpd_), sum(fnd_), sum(fpd_))\n",
        "\n",
        "acc_segmentation = segmentation_accuracy(sum(tp_), sum(tn_), sum(fp_), sum(fn_))\n",
        "recall_segmentation = segmentation_recall(sum(tp_), sum(fn_))\n",
        "precision_segmentation = segmentation_precision(sum(tp_), sum(fp_))\n",
        "DC_segmentation = segmentation_dice_coefficient(sum(tp_), sum(fn_), sum(fp_))\n",
        "\n",
        "F1_detection = (2*precision_detection*recall_detection)/(precision_detection+recall_detection)\n",
        "F2_detection = (5*precision_detection*recall_detection)/(4*precision_detection+recall_detection) \n",
        "\n",
        "TP_number = sum(tpd_)\n",
        "TN_number = sum(tnd_)\n",
        "FP_number = sum(fpd_)\n",
        "FN_number = sum(fnd_)\n",
        "\n",
        "# Print the evaluation results\n",
        "print('[Testing], DiceS: %.4f, DiceD: %.4f, AccuracyS: %.4f, AccuracyD: %.4f, RecallS: %.4f, RecallD: %.4f, PrecisionS: %.4f, PrecisionD: %.4f, F1_detection: %.4f, F2_detection: %.4f TP_Number: %.4f, TN_Number: %.4f, FP_Number: %.4f, FN_Number: %.4f' % (\n",
        "    DC_segmentation, \\\n",
        "    DC_detection, \\\n",
        "    acc_segmentation, \\\n",
        "    acc_detection, \\\n",
        "    recall_segmentation, \\\n",
        "    recall_detection, \\\n",
        "    precision_segmentation, \\\n",
        "    precision_detection, \\\n",
        "    F1_detection, \\\n",
        "    F2_detection, \\\n",
        "    TP_number, \\\n",
        "    TN_number, \\\n",
        "    FP_number, \\\n",
        "    FN_number, \\\n",
        "    ))\n",
        "\n",
        "writer_test_acc.add_scalar('Detection Accuracy', acc_detection)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO6swtNcwGkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    \n",
        "    encoder_params = list(filter(lambda kv: kv[0] in my_list, self.attUnet.named_parameters()))\n",
        "    \n",
        "    self.optimizer = optim.Adam([\n",
        "    {\"params\": encoder_params, \"lr\": 0.000001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.conv1.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.conv1.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.bn1.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.bn1.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.conv2.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.conv2.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.bn2.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.bn2.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.conv1.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.conv1.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.bn1.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.bn1.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.conv2.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.conv2.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.bn2.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.bn2.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.conv1.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.conv1.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.bn1.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.bn1.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.conv2.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.conv2.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.bn2.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.bn2.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.conv1.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.conv1.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.bn1.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.bn1.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.conv2.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.conv2.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.bn2.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.bn2.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.conv1.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.conv1.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.bn1.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.bn1.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.conv2.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.conv2.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.bn2.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.bn2.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.conv1.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.conv1.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.bn1.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.bn1.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.conv2.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.conv2.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.bn2.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.bn2.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv1.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.bn1.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.bn1.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv2.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.bn2.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.bn2.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv3.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.bn3.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.bn3.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv4.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.bn4.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.bn4.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.dupsample4.conv_w.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.dupsample4.conv_p.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.conv6.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.bn6.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.bn6.bias, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.final.weight, \"lr\": 0.00001},\n",
        "    { \"params\" : self.attUnet.module.final.bias, \"lr\": 0.0001}], self.lr, [self.beta1, self.beta2])\n",
        "\n",
        "    encoder_params = list(filter(lambda kv: kv[0] in my_list, self.attUnet.named_parameters()))\n",
        "    \n",
        "    self.optimizer = optim.Adam([\n",
        "    {\"params\": encoder_params, \"lr\": 0.000001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_1.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_2.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_3.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_1.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_2.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_3.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_3.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_3.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_3.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_3.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_3.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_3.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv1_3.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_1.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_2.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_2.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_2.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_2.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_2.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_2.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_2.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv2_2.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv3_1.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv3_1.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv3_1.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv3_1.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv3_1.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv3_1.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv3_1.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv3_1.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_4.conv1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_4.conv1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_4.bn1.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_4.bn1.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_4.conv2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_4.conv2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_4.bn2.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.conv0_4.bn2.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.final.weight, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.final.bias, \"lr\": 0.0001},\n",
        "    { \"params\" : self.attUnet.module.final.bias, \"lr\": 0.0001}], self.lr, [self.beta1, self.beta2])\n",
        "\n",
        "The number of parameters: 96001153\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}